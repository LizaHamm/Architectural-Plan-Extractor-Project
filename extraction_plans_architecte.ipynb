{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è Extraction de Plans Num√©riques depuis PDF d'Architecte\n",
        "\n",
        "## üìã Projet: Extraction Automatique de Plans Num√©riques Exploitables\n",
        "\n",
        "**Objectif**: Cr√©er un outil d'extraction automatique de plans num√©riques exploitables (format vectoriel ou BIM) √† partir de documents PDF d'architecte gr√¢ce √† la vision par ordinateur et au deep learning.\n",
        "\n",
        "**Livrable**: Notebook unique et complet avec pipeline end-to-end\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Guide Rapide d'Utilisation\n",
        "\n",
        "### ‚ö° D√©marrage Rapide (3 √©tapes)\n",
        "\n",
        "```python\n",
        "# 1. Ex√©cuter Section 1 (Setup) - OBLIGATOIRE\n",
        "# 2. Placer vos PDFs dans data/pdfs/\n",
        "# 3. Ex√©cuter ce code:\n",
        "\n",
        "pdf_path = \"data/pdfs/mon_plan.pdf\"\n",
        "result = process_pdf(pdf_path)  # Extraction\n",
        "annotations = labeler.label_image(result['saved_paths'][0])  # D√©tection\n",
        "labeler.visualize_annotations(result['saved_paths'][0], annotations)  # Visualiser\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### üìñ Documentation Compl√®te\n",
        "\n",
        "Voir le fichier **`GUIDE_UTILISATION.md`** pour:\n",
        "- Installation d√©taill√©e\n",
        "- Configuration compl√®te\n",
        "- Workflow √©tape par √©tape\n",
        "- D√©tection et extraction\n",
        "- Visualisation des r√©sultats\n",
        "- D√©pannage\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Architecture du Pipeline\n",
        "\n",
        "```\n",
        "PDF Input ‚Üí Extraction Images ‚Üí Preprocessing ‚Üí D√©tection (YOLO) ‚Üí Segmentation ‚Üí \n",
        "Post-processing ‚Üí Export Vectoriel/BIM ‚Üí Visualisation ‚Üí Stockage Snowflake\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Structure du Notebook\n",
        "\n",
        "1. **Setup et Imports** - Configuration environnement ‚ö†Ô∏è OBLIGATOIRE\n",
        "2. **Snowflake Configuration** - Connexion et sch√©mas (optionnel)\n",
        "3. **Extraction PDF** - Conversion PDF ‚Üí Images ‚ö†Ô∏è OBLIGATOIRE\n",
        "4. **G√©n√©ration Donn√©es LLM** - Augmentation avec IA g√©n√©rative (optionnel)\n",
        "5. **Labellisation** - Automatique et manuelle ‚ö†Ô∏è OBLIGATOIRE\n",
        "6. **Entra√Ænement YOLO** - Fine-tuning mod√®les (optionnel, long)\n",
        "7. **Inf√©rence** - Extraction sur nouveaux PDF ‚ö†Ô∏è OBLIGATOIRE\n",
        "8. **Visualisation** - Dashboard interactif ‚ö†Ô∏è OBLIGATOIRE\n",
        "9. **Tests et Validation** - M√©triques et qualit√© (recommand√©)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1: Setup et Imports\n",
        "\n",
        "## 1.1 Installation des d√©pendances (√† ex√©cuter une seule fois)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 T√©l√©chargement des Mod√®les YOLO (Optionnel - Premi√®re Utilisation)\n",
        "\n",
        "Les mod√®les YOLO seront t√©l√©charg√©s automatiquement lors de leur premi√®re utilisation, mais vous pouvez les pr√©-t√©l√©charger ici pour √©viter les attentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ T√©l√©chargement des mod√®les YOLO...\n",
            "‚è≥ Cela peut prendre quelques minutes lors du premier t√©l√©chargement...\n",
            "\n",
            "üì• T√©l√©chargement: yolo11n.pt (D√©tection)...\n",
            "‚ùå Erreur lors du t√©l√©chargement: name 'YOLO' is not defined\n",
            "\n",
            "üí° Les mod√®les seront t√©l√©charg√©s automatiquement lors de leur premi√®re utilisation\n",
            "   Vous pouvez continuer avec le notebook, le t√©l√©chargement se fera dans la Section 5\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# T√âL√âCHARGEMENT DES MOD√àLES YOLO\n",
        "# ============================================================================\n",
        "# \n",
        "# IMPORTANT: Les mod√®les seront t√©l√©charg√©s automatiquement lors de leur \n",
        "# premi√®re utilisation dans la Section 5. Cette cellule est optionnelle \n",
        "# mais recommand√©e pour pr√©-t√©l√©charger les mod√®les.\n",
        "#\n",
        "# M√©thodes alternatives:\n",
        "# 1. Ex√©cuter cette cellule dans le notebook (RECOMMAND√â)\n",
        "# 2. Cr√©er un script Python s√©par√© (voir ci-dessous)\n",
        "# 3. Utiliser PowerShell/terminal (voir ci-dessous)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîÑ T√©l√©chargement des mod√®les YOLO...\")\n",
        "print(\"‚è≥ Cela peut prendre quelques minutes lors du premier t√©l√©chargement...\\n\")\n",
        "\n",
        "try:\n",
        "    # Mod√®le d√©tection (le plus utilis√©)\n",
        "    print(\"üì• T√©l√©chargement: yolo11n.pt (D√©tection)...\")\n",
        "    model_det = YOLO(\"yolo11n.pt\")\n",
        "    print(\"   ‚úÖ Mod√®le d√©tection t√©l√©charg√©\\n\")\n",
        "    \n",
        "    # Mod√®le segmentation\n",
        "    print(\"üì• T√©l√©chargement: yolo11n-seg.pt (Segmentation)...\")\n",
        "    model_seg = YOLO(\"yolo11n-seg.pt\")\n",
        "    print(\"   ‚úÖ Mod√®le segmentation t√©l√©charg√©\\n\")\n",
        "    \n",
        "    # Mod√®le keypoints\n",
        "    print(\"üì• T√©l√©chargement: yolo11n-pose.pt (Keypoints)...\")\n",
        "    model_kpt = YOLO(\"yolo11n-pose.pt\")\n",
        "    print(\"   ‚úÖ Mod√®le keypoints t√©l√©charg√©\\n\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"‚úÖ TOUS LES MOD√àLES SONT T√âL√âCHARG√âS!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nüí° Les mod√®les sont sauvegard√©s dans:\")\n",
        "    print(f\"   {Path.home() / '.ultralytics'}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors du t√©l√©chargement: {e}\")\n",
        "    print(\"\\nüí° Les mod√®les seront t√©l√©charg√©s automatiquement lors de leur premi√®re utilisation\")\n",
        "    print(\"   Vous pouvez continuer avec le notebook, le t√©l√©chargement se fera dans la Section 5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù M√©thodes Alternatives de T√©l√©chargement\n",
        "\n",
        "Si vous pr√©f√©rez t√©l√©charger les mod√®les en dehors du notebook:\n",
        "\n",
        "**Option 1: Script Python s√©par√©**\n",
        "```bash\n",
        "# Cr√©er un fichier download_models.py\n",
        "# Puis ex√©cuter:\n",
        "python download_models.py\n",
        "```\n",
        "\n",
        "**Option 2: PowerShell/CMD (Windows)**\n",
        "```powershell\n",
        "# Activer l'environnement virtuel d'abord\n",
        "venv\\Scripts\\activate\n",
        "\n",
        "# Puis ex√©cuter Python directement\n",
        "python -c \"from ultralytics import YOLO; YOLO('yolo11n.pt'); YOLO('yolo11n-seg.pt'); YOLO('yolo11n-pose.pt')\"\n",
        "```\n",
        "\n",
        "**Option 3: Terminal Linux/Mac**\n",
        "```bash\n",
        "# Activer l'environnement virtuel\n",
        "source venv/bin/activate\n",
        "\n",
        "# Ex√©cuter\n",
        "python -c \"from ultralytics import YOLO; YOLO('yolo11n.pt'); YOLO('yolo11n-seg.pt'); YOLO('yolo11n-pose.pt')\"\n",
        "```\n",
        "\n",
        "**Note**: Les mod√®les seront t√©l√©charg√©s automatiquement lors de leur premi√®re utilisation dans la Section 5 si vous ne les t√©l√©chargez pas maintenant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (12.1.0)\n",
            "Requirement already satisfied: scikit-image in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.25.2)\n",
            "Requirement already satisfied: ultralytics in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (8.3.249)\n",
            "Requirement already satisfied: torch in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (2.9.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.24.1)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from opencv-python) (2.2.6)\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image) (2025.5.10)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image) (2.37.2)\n",
            "Requirement already satisfied: packaging>=21 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image) (1.15.3)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: polars>=0.20.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ultralytics) (1.36.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ultralytics) (2.32.5)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ultralytics) (7.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ultralytics) (3.10.8)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from torch) (2025.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: polars-runtime-32==1.36.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from polars>=0.20.0->ultralytics) (1.36.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyMuPDF in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (1.26.7)\n",
            "Requirement already satisfied: pdf2image in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (1.17.0)\n",
            "Requirement already satisfied: pdfplumber in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.11.9)\n",
            "Requirement already satisfied: pillow in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pdf2image) (12.1.0)\n",
            "Requirement already satisfied: pdfminer.six==20251230 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pdfplumber) (20251230)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pdfplumber) (5.3.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (46.0.3)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.13.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (4.15.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (2.14.0)\n",
            "Requirement already satisfied: anthropic in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.75.0)\n",
            "Requirement already satisfied: transformers in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (4.57.3)\n",
            "Requirement already satisfied: langchain in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (2.12.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: requests in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests->transformers) (2.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: diffusers in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.36.0)\n",
            "Requirement already satisfied: controlnet-aux in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.0.10)\n",
            "Requirement already satisfied: filelock in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (3.20.2)\n",
            "Requirement already satisfied: importlib_metadata in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (8.7.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (2.2.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (2025.11.3)\n",
            "Requirement already satisfied: requests in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (2.32.5)\n",
            "Requirement already satisfied: Pillow in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (12.1.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from diffusers) (0.7.0)\n",
            "Requirement already satisfied: torch in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from controlnet-aux) (2.9.1)\n",
            "Requirement already satisfied: scikit-image in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from controlnet-aux) (0.25.2)\n",
            "Requirement already satisfied: einops in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from controlnet-aux) (0.8.1)\n",
            "Requirement already satisfied: opencv-python-headless in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from controlnet-aux) (4.12.0.88)\n",
            "Requirement already satisfied: torchvision in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from controlnet-aux) (0.24.1)\n",
            "Requirement already satisfied: timm in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from controlnet-aux) (1.0.24)\n",
            "Requirement already satisfied: scipy in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from controlnet-aux) (1.15.3)\n",
            "Requirement already satisfied: certifi in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpx<1.0.0->diffusers) (2026.1.4)\n",
            "Requirement already satisfied: anyio in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpx<1.0.0->diffusers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpx<1.0.0->diffusers) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpx<1.0.0->diffusers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->diffusers) (0.16.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (25.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (4.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (4.67.1)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests->diffusers) (2.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from requests->diffusers) (3.4.4)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image->controlnet-aux) (2.37.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image->controlnet-aux) (0.4)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image->controlnet-aux) (2025.5.10)\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from scikit-image->controlnet-aux) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from torch->controlnet-aux) (3.1.6)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from torch->controlnet-aux) (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->controlnet-aux) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<2.0,>=0.34.0->diffusers) (0.4.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from anyio->httpx<1.0.0->diffusers) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from jinja2->torch->controlnet-aux) (3.0.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: snowflake-connector-python in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (4.1.1)\n",
            "Requirement already satisfied: snowflake-sqlalchemy in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (1.8.2)\n",
            "Requirement already satisfied: pytz in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (25.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (4.15.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (3.20.2)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (2026.1.4)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (4.5.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (3.4.4)\n",
            "Requirement already satisfied: tomlkit in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (0.13.3)\n",
            "Requirement already satisfied: cryptography>=44.0.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (46.0.3)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (1.5.1)\n",
            "Requirement already satisfied: boto3>=1.24 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (1.42.23)\n",
            "Requirement already satisfied: botocore>=1.24 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (1.42.23)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: idna<4,>=3.7 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (3.11)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (2.32.5)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=24.0.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-connector-python) (25.3.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.19 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from snowflake-sqlalchemy) (2.0.45)\n",
            "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from boto3>=1.24->snowflake-connector-python) (0.16.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from botocore>=1.24->snowflake-connector-python) (2.6.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from cryptography>=44.0.1->snowflake-connector-python) (2.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from sqlalchemy>=1.4.19->snowflake-sqlalchemy) (3.3.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=44.0.1->snowflake-connector-python) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (3.10.8)\n",
            "Requirement already satisfied: plotly in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (6.5.0)\n",
            "Requirement already satisfied: seaborn in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (12.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from plotly) (2.15.0)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ezdxf in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (1.4.3)\n",
            "Requirement already satisfied: ifcopenshell in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (0.8.4.post1)\n",
            "Requirement already satisfied: numpy in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ezdxf) (2.2.6)\n",
            "Requirement already satisfied: typing_extensions>=4.6.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ezdxf) (4.15.0)\n",
            "Requirement already satisfied: fonttools in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ezdxf) (4.61.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ezdxf) (3.3.1)\n",
            "Requirement already satisfied: shapely in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ifcopenshell) (2.1.2)\n",
            "Requirement already satisfied: isodate in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ifcopenshell) (0.7.2)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ifcopenshell) (2.9.0.post0)\n",
            "Requirement already satisfied: lark in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ifcopenshell) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from python-dateutil->ifcopenshell) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (2.2.6)\n",
            "Requirement already satisfied: scipy in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (1.15.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (8.1.8)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipywidgets) (3.0.16)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipywidgets) (8.38.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipywidgets) (4.0.15)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\dell\\archiproject\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Installation des packages n√©cessaires (d√©commenter si n√©cessaire)\n",
        "!pip install opencv-python pillow scikit-image ultralytics torch torchvision\n",
        "!pip install PyMuPDF pdf2image pdfplumber\n",
        "!pip install openai anthropic transformers langchain\n",
        "!pip install diffusers controlnet-aux\n",
        "!pip install snowflake-connector-python snowflake-sqlalchemy\n",
        "!pip install matplotlib plotly seaborn\n",
        "!pip install ezdxf ifcopenshell\n",
        "!pip install pandas numpy scipy\n",
        "!pip install ipywidgets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Imports standards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tous les imports sont r√©ussis!\n",
            "PyTorch version: 2.9.1+cpu\n",
            "CUDA disponible: False\n"
          ]
        }
      ],
      "source": [
        "# Imports standards\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Computer Vision\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import skimage\n",
        "from skimage import filters, morphology, transform\n",
        "\n",
        "# PDF Processing\n",
        "import fitz  # PyMuPDF\n",
        "from pdf2image import convert_from_path\n",
        "import pdfplumber\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torchvision\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils import LOGGER\n",
        "\n",
        "# LLM and Generative AI\n",
        "try:\n",
        "    from anthropic import Anthropic\n",
        "    ANTHROPIC_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ANTHROPIC_AVAILABLE = False\n",
        "    print(\"Anthropic non disponible, utiliser un mod√®le local\")\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "# Snowflake\n",
        "try:\n",
        "    import snowflake.connector\n",
        "    from snowflake.connector.pandas_tools import write_pandas\n",
        "    SNOWFLAKE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SNOWFLAKE_AVAILABLE = False\n",
        "    print(\"Snowflake non disponible\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import seaborn as sns\n",
        "\n",
        "# Export formats\n",
        "try:\n",
        "    import ezdxf\n",
        "    EZDXF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    EZDXF_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import ifcopenshell\n",
        "    IFC_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IFC_AVAILABLE = False\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Tous les imports sont r√©ussis!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Configuration globale et variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration initialis√©e!\n",
            "üìÅ R√©pertoires cr√©√©s: ['data', 'models', 'output']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION GLOBALE\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration centrale du projet\"\"\"\n",
        "    \n",
        "    # Chemins\n",
        "    BASE_DIR = Path(\".\")\n",
        "    DATA_DIR = BASE_DIR / \"data\"\n",
        "    PDF_DIR = DATA_DIR / \"pdfs\"\n",
        "    IMAGES_DIR = DATA_DIR / \"images\"\n",
        "    ANNOTATIONS_DIR = DATA_DIR / \"annotations\"\n",
        "    MODELS_DIR = BASE_DIR / \"models\"\n",
        "    OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "    \n",
        "    # Cr√©ation des r√©pertoires\n",
        "    for dir_path in [DATA_DIR, PDF_DIR, IMAGES_DIR, ANNOTATIONS_DIR, MODELS_DIR, OUTPUT_DIR]:\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Param√®tres PDF\n",
        "    PDF_DPI = 300  # R√©solution pour conversion PDF ‚Üí Image\n",
        "    PDF_SCALE = 2.0  # Facteur d'√©chelle\n",
        "    \n",
        "    # Param√®tres preprocessing\n",
        "    IMAGE_SIZE = (1024, 1024)  # Taille standardis√©e\n",
        "    BINARY_THRESHOLD = 127  # Seuil binarisation\n",
        "    \n",
        "    # Classes YOLO (√©l√©ments architecturaux)\n",
        "    CLASSES = {\n",
        "        0: \"porte\",\n",
        "        1: \"fenetre\",\n",
        "        2: \"mur\",\n",
        "        3: \"escalier\",\n",
        "        4: \"colonne\",\n",
        "        5: \"sanitaire\",\n",
        "        6: \"cuisine\",\n",
        "        7: \"couloir\",\n",
        "        8: \"piece\",\n",
        "        9: \"balcon\"\n",
        "    }\n",
        "    \n",
        "    # Param√®tres YOLO\n",
        "    YOLO_MODEL_DETECTION = \"yolo11n.pt\"  # Mod√®le pr√©-entra√Æn√© d√©tection\n",
        "    YOLO_MODEL_SEGMENTATION = \"yolo11n-seg.pt\"  # Mod√®le segmentation\n",
        "    YOLO_MODEL_KEYPOINTS = \"yolo11n-pose.pt\"  # Mod√®le keypoints\n",
        "    YOLO_IMG_SIZE = 640\n",
        "    YOLO_CONF_THRESHOLD = 0.25\n",
        "    YOLO_IOU_THRESHOLD = 0.45\n",
        "    \n",
        "    # Param√®tres LLM\n",
        "    LLM_MODEL = \"claude-3-5-sonnet-20241022\"  # Anthropic Claude par d√©faut\n",
        "    LLM_TEMPERATURE = 0.7\n",
        "    NUM_SCENARIOS = 50  # Nombre de sc√©narios √† g√©n√©rer\n",
        "    \n",
        "    # Param√®tres Snowflake\n",
        "    SNOWFLAKE_ACCOUNT = os.getenv(\"SNOWFLAKE_ACCOUNT\", \"\")\n",
        "    SNOWFLAKE_USER = os.getenv(\"SNOWFLAKE_USER\", \"\")\n",
        "    SNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\", \"\")\n",
        "    SNOWFLAKE_WAREHOUSE = os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\")\n",
        "    SNOWFLAKE_DATABASE = os.getenv(\"SNOWFLAKE_DATABASE\", \"ARCHITECTURE_DB\")\n",
        "    SNOWFLAKE_SCHEMA = os.getenv(\"SNOWFLAKE_SCHEMA\", \"PLANS_SCHEMA\")\n",
        "    \n",
        "    # Param√®tres g√©n√©ration donn√©es\n",
        "    AUGMENTATION_ROTATIONS = [90, 180, 270]\n",
        "    AUGMENTATION_FLIP = True\n",
        "    AUGMENTATION_SCALE = [0.8, 1.0, 1.2]\n",
        "    AUGMENTATION_NOISE = True\n",
        "\n",
        "# Instance globale de configuration\n",
        "config = Config()\n",
        "\n",
        "print(\"‚úÖ Configuration initialis√©e!\")\n",
        "print(f\"üìÅ R√©pertoires cr√©√©s: {[str(d) for d in [config.DATA_DIR, config.MODELS_DIR, config.OUTPUT_DIR]]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Erreur connexion Snowflake: 290404 (08001): 404 Not Found: post yhurmit-us29400.snowflakecomputing.com.snowflakecomputing.com:443/session/v1/login-request\n",
            "üí° Mode simulation activ√© (pas de connexion r√©elle)\n",
            "üí° Le notebook fonctionnera en mode simulation (sans Snowflake r√©el)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONNEXION SNOWFLAKE\n",
        "# ============================================================================\n",
        "\n",
        "class SnowflakeManager:\n",
        "    \"\"\"Gestionnaire de connexion et op√©rations Snowflake\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.conn = None\n",
        "        self.cursor = None\n",
        "        \n",
        "    def connect(self):\n",
        "        \"\"\"√âtablit la connexion √† Snowflake\"\"\"\n",
        "        if not SNOWFLAKE_AVAILABLE:\n",
        "            print(\"‚ö†Ô∏è Snowflake non disponible, mode simulation activ√©\")\n",
        "            return False\n",
        "            \n",
        "        try:\n",
        "            self.conn = snowflake.connector.connect(\n",
        "                user=self.config.SNOWFLAKE_USER,\n",
        "                password=self.config.SNOWFLAKE_PASSWORD,\n",
        "                account=self.config.SNOWFLAKE_ACCOUNT,\n",
        "                warehouse=self.config.SNOWFLAKE_WAREHOUSE,\n",
        "                database=self.config.SNOWFLAKE_DATABASE,\n",
        "                schema=self.config.SNOWFLAKE_SCHEMA\n",
        "            )\n",
        "            self.cursor = self.conn.cursor()\n",
        "            print(\"‚úÖ Connexion Snowflake r√©ussie!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur connexion Snowflake: {e}\")\n",
        "            print(\"üí° Mode simulation activ√© (pas de connexion r√©elle)\")\n",
        "            return False\n",
        "    \n",
        "    def execute_query(self, query: str, fetch: bool = True):\n",
        "        \"\"\"Ex√©cute une requ√™te SQL\"\"\"\n",
        "        if not self.conn:\n",
        "            print(f\"‚ö†Ô∏è Mode simulation - Requ√™te: {query[:100]}...\")\n",
        "            return None\n",
        "        try:\n",
        "            self.cursor.execute(query)\n",
        "            if fetch:\n",
        "                return self.cursor.fetchall()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur requ√™te: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def create_schema(self):\n",
        "        \"\"\"Cr√©e le sch√©ma et les tables n√©cessaires\"\"\"\n",
        "        queries = [\n",
        "            f\"CREATE DATABASE IF NOT EXISTS {self.config.SNOWFLAKE_DATABASE}\",\n",
        "            f\"USE DATABASE {self.config.SNOWFLAKE_DATABASE}\",\n",
        "            f\"CREATE SCHEMA IF NOT EXISTS {self.config.SNOWFLAKE_SCHEMA}\",\n",
        "            f\"USE SCHEMA {self.config.SNOWFLAKE_SCHEMA}\",\n",
        "            \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS plans_metadata (\n",
        "                id VARCHAR(255) PRIMARY KEY,\n",
        "                nom_fichier VARCHAR(500),\n",
        "                date_upload TIMESTAMP,\n",
        "                dimensions_width INT,\n",
        "                dimensions_height INT,\n",
        "                nombre_pages INT,\n",
        "                taille_fichier_kb FLOAT,\n",
        "                metadata_json VARIANT,\n",
        "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        "            )\n",
        "            \"\"\",\n",
        "            \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS annotations (\n",
        "                id VARCHAR(255) PRIMARY KEY,\n",
        "                plan_id VARCHAR(255),\n",
        "                classe VARCHAR(50),\n",
        "                bbox_x FLOAT,\n",
        "                bbox_y FLOAT,\n",
        "                bbox_width FLOAT,\n",
        "                bbox_height FLOAT,\n",
        "                confidence FLOAT,\n",
        "                segmentation_points VARIANT,\n",
        "                keypoints VARIANT,\n",
        "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "                FOREIGN KEY (plan_id) REFERENCES plans_metadata(id)\n",
        "            )\n",
        "            \"\"\",\n",
        "            \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS model_versions (\n",
        "                id VARCHAR(255) PRIMARY KEY,\n",
        "                version VARCHAR(50),\n",
        "                model_type VARCHAR(50),\n",
        "                mAP FLOAT,\n",
        "                precision FLOAT,\n",
        "                recall FLOAT,\n",
        "                date_entrainement TIMESTAMP,\n",
        "                chemin_modele VARCHAR(500),\n",
        "                hyperparameters VARIANT,\n",
        "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        "            )\n",
        "            \"\"\",\n",
        "            \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS scenarios_llm (\n",
        "                scenario_id VARCHAR(255) PRIMARY KEY,\n",
        "                parametres_json VARIANT,\n",
        "                image_generee_url VARCHAR(1000),\n",
        "                type_plan VARCHAR(100),\n",
        "                nombre_elements INT,\n",
        "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        "            )\n",
        "            \"\"\"\n",
        "        ]\n",
        "        \n",
        "        for query in queries:\n",
        "            self.execute_query(query, fetch=False)\n",
        "        \n",
        "        print(\"‚úÖ Sch√©ma Snowflake cr√©√© avec succ√®s!\")\n",
        "    \n",
        "    def insert_plan_metadata(self, plan_data: Dict):\n",
        "        \"\"\"Ins√®re les m√©tadonn√©es d'un plan\"\"\"\n",
        "        query = f\"\"\"\n",
        "        INSERT INTO plans_metadata \n",
        "        (id, nom_fichier, date_upload, dimensions_width, dimensions_height, \n",
        "         nombre_pages, taille_fichier_kb, metadata_json)\n",
        "        VALUES (\n",
        "            '{plan_data['id']}',\n",
        "            '{plan_data['nom_fichier']}',\n",
        "            '{plan_data['date_upload']}',\n",
        "            {plan_data['dimensions_width']},\n",
        "            {plan_data['dimensions_height']},\n",
        "            {plan_data['nombre_pages']},\n",
        "            {plan_data['taille_fichier_kb']},\n",
        "            PARSE_JSON('{json.dumps(plan_data.get('metadata_json', {}))}')\n",
        "        )\n",
        "        \"\"\"\n",
        "        return self.execute_query(query, fetch=False)\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Ferme la connexion\"\"\"\n",
        "        if self.cursor:\n",
        "            self.cursor.close()\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "        print(\"‚úÖ Connexion Snowflake ferm√©e\")\n",
        "\n",
        "# Initialisation\n",
        "snowflake_mgr = SnowflakeManager(config)\n",
        "snowflake_connected = snowflake_mgr.connect()\n",
        "\n",
        "if snowflake_connected:\n",
        "    snowflake_mgr.create_schema()\n",
        "else:\n",
        "    print(\"üí° Le notebook fonctionnera en mode simulation (sans Snowflake r√©el)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3: Extraction et Preprocessing PDF\n",
        "\n",
        "## ‚ùì Pourquoi convertir les PDF en images ?\n",
        "\n",
        "### Raisons techniques principales :\n",
        "\n",
        "1. **üì∏ Format natif des mod√®les de Deep Learning**\n",
        "   - YOLO, et tous les mod√®les de computer vision, travaillent avec des **matrices de pixels** (images)\n",
        "   - PyTorch/TensorFlow n√©cessitent des formats image (PNG, JPEG) pour les tensors\n",
        "   - Les PDFs sont des formats **vectoriels/textuels complexes** non compatibles directement\n",
        "\n",
        "2. **üîß Preprocessing n√©cessaire**\n",
        "   - Binarisation, d√©noisage, correction d'orientation ‚Üí n√©cessitent des images\n",
        "   - OpenCV et scikit-image travaillent sur des arrays NumPy (repr√©sentation d'images)\n",
        "   - Les op√©rations morphologiques (√©rosion, dilatation) s'appliquent aux pixels\n",
        "\n",
        "3. **üìä Annotations YOLO**\n",
        "   - Format YOLO: coordonn√©es normalis√©es bas√©es sur les **dimensions de l'image** (width, height)\n",
        "   - Les bounding boxes sont d√©finies en pixels, pas en coordonn√©es vectorielles PDF\n",
        "   - La segmentation n√©cessite des masques d'images (matrices binaires)\n",
        "\n",
        "4. **‚ö° Performance et standardisation**\n",
        "   - Images = format universel et standardis√© pour computer vision\n",
        "   - Plus rapide √† traiter qu'un parsing complexe de PDF\n",
        "   - Compatible avec toutes les biblioth√®ques de vision par ordinateur\n",
        "\n",
        "5. **üîÑ Pipeline coh√©rent**\n",
        "   - Toutes les √©tapes (augmentation, labellisation, entra√Ænement) utilisent des images\n",
        "   - Facilite le workflow: PDF ‚Üí Image ‚Üí Traitement ‚Üí Export vectoriel\n",
        "\n",
        "### ‚ö†Ô∏è Note importante :\n",
        "- La conversion PDF ‚Üí Image est une **√©tape interm√©diaire n√©cessaire**\n",
        "- On **reconvertit ensuite en format vectoriel** (DXF, IFC) √† la fin du pipeline\n",
        "- On ne perd pas la qualit√©: conversion haute r√©solution (300 DPI minimum)\n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 Extraction des pages PDF en images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Exemple d'extraction PDF:\n",
            "Pour tester, placez vos fichiers PDF dans le dossier data/pdfs/\n",
            "‚úÖ Extracteur PDF initialis√©!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# EXTRACTION PDF ‚Üí IMAGES\n",
        "# ============================================================================\n",
        "\n",
        "class PDFExtractor:\n",
        "    \"\"\"Extracteur de pages PDF vers images\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    \n",
        "    def extract_pages_pymupdf(self, pdf_path: str) -> List[Image.Image]:\n",
        "        \"\"\"\n",
        "        Extrait les pages d'un PDF en images haute qualit√© avec PyMuPDF\n",
        "        \n",
        "        Args:\n",
        "            pdf_path: Chemin vers le fichier PDF\n",
        "            \n",
        "        Returns:\n",
        "            Liste d'images PIL\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            print(f\"üìÑ PDF ouvert: {len(doc)} pages\")\n",
        "            \n",
        "            for page_num in range(len(doc)):\n",
        "                page = doc[page_num]\n",
        "                \n",
        "                # Matrice de transformation pour haute r√©solution\n",
        "                mat = fitz.Matrix(self.config.PDF_SCALE, self.config.PDF_SCALE)\n",
        "                pix = page.get_pixmap(matrix=mat)\n",
        "                \n",
        "                # Conversion en PIL Image\n",
        "                img_data = pix.tobytes(\"png\")\n",
        "                img = Image.open(BytesIO(img_data))\n",
        "                images.append(img)\n",
        "                \n",
        "                print(f\"  ‚úì Page {page_num + 1} extraite: {img.size}\")\n",
        "            \n",
        "            doc.close()\n",
        "            return images\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur extraction PyMuPDF: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def extract_pages_pdf2image(self, pdf_path: str) -> List[Image.Image]:\n",
        "        \"\"\"\n",
        "        Alternative: Extraction avec pdf2image (n√©cessite poppler)\n",
        "        \n",
        "        Args:\n",
        "            pdf_path: Chemin vers le fichier PDF\n",
        "            \n",
        "        Returns:\n",
        "            Liste d'images PIL\n",
        "        \"\"\"\n",
        "        try:\n",
        "            images = convert_from_path(\n",
        "                pdf_path,\n",
        "                dpi=self.config.PDF_DPI,\n",
        "                fmt='png'\n",
        "            )\n",
        "            print(f\"üìÑ {len(images)} pages extraites avec pdf2image\")\n",
        "            return images\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur extraction pdf2image: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def save_extracted_images(self, images: List[Image.Image], \n",
        "                             pdf_name: str, output_dir: Path) -> List[str]:\n",
        "        \"\"\"\n",
        "        Sauvegarde les images extraites\n",
        "        \n",
        "        Args:\n",
        "            images: Liste d'images PIL\n",
        "            pdf_name: Nom du PDF (sans extension)\n",
        "            output_dir: R√©pertoire de sortie\n",
        "            \n",
        "        Returns:\n",
        "            Liste des chemins des images sauvegard√©es\n",
        "        \"\"\"\n",
        "        saved_paths = []\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        for idx, img in enumerate(images):\n",
        "            img_path = output_dir / f\"{pdf_name}_page_{idx+1:03d}.png\"\n",
        "            img.save(img_path, \"PNG\", quality=95)\n",
        "            saved_paths.append(str(img_path))\n",
        "            print(f\"  üíæ Image sauvegard√©e: {img_path.name}\")\n",
        "        \n",
        "        return saved_paths\n",
        "    \n",
        "    def get_pdf_metadata(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Extrait les m√©tadonn√©es d'un PDF\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            metadata = doc.metadata\n",
        "            doc.close()\n",
        "            \n",
        "            file_size = os.path.getsize(pdf_path) / 1024  # KB\n",
        "            \n",
        "            return {\n",
        "                'nombre_pages': len(doc),\n",
        "                'taille_fichier_kb': file_size,\n",
        "                'titre': metadata.get('title', ''),\n",
        "                'auteur': metadata.get('author', ''),\n",
        "                'sujet': metadata.get('subject', ''),\n",
        "                'date_creation': metadata.get('creationDate', '')\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur m√©tadonn√©es: {e}\")\n",
        "            return {}\n",
        "\n",
        "# Test d'extraction\n",
        "extractor = PDFExtractor(config)\n",
        "\n",
        "# Exemple d'utilisation (remplacer par vos PDFs)\n",
        "print(\"üìù Exemple d'extraction PDF:\")\n",
        "print(\"Pour tester, placez vos fichiers PDF dans le dossier data/pdfs/\")\n",
        "\n",
        "# Fonction helper pour traiter un PDF\n",
        "def process_pdf(pdf_path: str) -> Dict:\n",
        "    \"\"\"Traite un PDF complet\"\"\"\n",
        "    pdf_name = Path(pdf_path).stem\n",
        "    \n",
        "    # Extraction\n",
        "    images = extractor.extract_pages_pymupdf(pdf_path)\n",
        "    if not images:\n",
        "        images = extractor.extract_pages_pdf2image(pdf_path)\n",
        "    \n",
        "    if not images:\n",
        "        return None\n",
        "    \n",
        "    # Sauvegarde\n",
        "    saved_paths = extractor.save_extracted_images(\n",
        "        images, pdf_name, config.IMAGES_DIR\n",
        "    )\n",
        "    \n",
        "    # M√©tadonn√©es\n",
        "    metadata = extractor.get_pdf_metadata(pdf_path)\n",
        "    first_img = images[0]\n",
        "    \n",
        "    # Stockage Snowflake\n",
        "    plan_data = {\n",
        "        'id': f\"plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        'nom_fichier': Path(pdf_path).name,\n",
        "        'date_upload': datetime.now().isoformat(),\n",
        "        'dimensions_width': first_img.width,\n",
        "        'dimensions_height': first_img.height,\n",
        "        'nombre_pages': len(images),\n",
        "        'taille_fichier_kb': metadata.get('taille_fichier_kb', 0),\n",
        "        'metadata_json': metadata\n",
        "    }\n",
        "    \n",
        "    if snowflake_connected:\n",
        "        snowflake_mgr.insert_plan_metadata(plan_data)\n",
        "    \n",
        "    return {\n",
        "        'plan_data': plan_data,\n",
        "        'images': images,\n",
        "        'saved_paths': saved_paths\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Extracteur PDF initialis√©!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Preprocesseur initialis√©!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PREPROCESSING IMAGES\n",
        "# ============================================================================\n",
        "\n",
        "class ImagePreprocessor:\n",
        "    \"\"\"Pr√©processeur d'images pour plans architecturaux\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    \n",
        "    def detect_orientation(self, image: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        D√©tecte l'orientation de l'image (0, 90, 180, 270 degr√©s)\n",
        "        Utilise la d√©tection de texte/contours pour d√©terminer l'orientation\n",
        "        \"\"\"\n",
        "        # Conversion en niveaux de gris\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
        "        \n",
        "        # D√©tection de contours\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, \n",
        "                                minLineLength=50, maxLineGap=10)\n",
        "        \n",
        "        if lines is not None and len(lines) > 0:\n",
        "            angles = []\n",
        "            for line in lines:\n",
        "                x1, y1, x2, y2 = line[0]\n",
        "                angle = np.degrees(np.arctan2(y2 - y1, x2 - x1))\n",
        "                angles.append(angle)\n",
        "            \n",
        "            # Angle dominant\n",
        "            angle_hist, _ = np.histogram(angles, bins=36, range=(-90, 90))\n",
        "            dominant_angle = np.argmax(angle_hist) * 5 - 90\n",
        "            \n",
        "            # Normalisation √† 0, 90, 180, 270\n",
        "            if abs(dominant_angle) < 45:\n",
        "                return 0\n",
        "            elif 45 <= dominant_angle < 135:\n",
        "                return 90\n",
        "            elif -135 <= dominant_angle < -45:\n",
        "                return -90\n",
        "            else:\n",
        "                return 180\n",
        "        \n",
        "        return 0\n",
        "    \n",
        "    def correct_orientation(self, image: Image.Image) -> Image.Image:\n",
        "        \"\"\"Corrige l'orientation de l'image\"\"\"\n",
        "        img_array = np.array(image)\n",
        "        angle = self.detect_orientation(img_array)\n",
        "        \n",
        "        if angle != 0:\n",
        "            # Rotation\n",
        "            if angle == 90:\n",
        "                return image.rotate(-90, expand=True)\n",
        "            elif angle == -90 or angle == 270:\n",
        "                return image.rotate(90, expand=True)\n",
        "            elif angle == 180:\n",
        "                return image.rotate(180, expand=True)\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def binarize_otsu(self, image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Binarisation avec m√©thode d'Otsu\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "        \n",
        "        # Binarisation Otsu\n",
        "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        return binary\n",
        "    \n",
        "    def binarize_adaptive(self, image: np.ndarray, block_size: int = 11, \n",
        "                         C: int = 2) -> np.ndarray:\n",
        "        \"\"\"Binarisation adaptative\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "        \n",
        "        binary = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
        "            cv2.THRESH_BINARY, block_size, C\n",
        "        )\n",
        "        return binary\n",
        "    \n",
        "    def denoise(self, image: np.ndarray, method: str = 'morphological') -> np.ndarray:\n",
        "        \"\"\"\n",
        "        D√©noisage de l'image\n",
        "        \n",
        "        Args:\n",
        "            image: Image binaris√©e\n",
        "            method: 'morphological' ou 'gaussian'\n",
        "        \"\"\"\n",
        "        if method == 'morphological':\n",
        "            # Op√©rations morphologiques\n",
        "            kernel = np.ones((3, 3), np.uint8)\n",
        "            denoised = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
        "            denoised = cv2.morphologyEx(denoised, cv2.MORPH_OPEN, kernel)\n",
        "            return denoised\n",
        "        \n",
        "        elif method == 'gaussian':\n",
        "            # Filtre gaussien (pour images non binaris√©es)\n",
        "            return cv2.GaussianBlur(image, (5, 5), 0)\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def normalize_size(self, image: Image.Image, target_size: Tuple[int, int] = None) -> Image.Image:\n",
        "        \"\"\"Normalise la taille de l'image\"\"\"\n",
        "        if target_size is None:\n",
        "            target_size = self.config.IMAGE_SIZE\n",
        "        \n",
        "        # Resize en conservant le ratio d'aspect\n",
        "        image.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
        "        \n",
        "        # Cr√©ation d'une nouvelle image avec la taille cible\n",
        "        new_img = Image.new('RGB', target_size, color='white')\n",
        "        new_img.paste(image, ((target_size[0] - image.width) // 2,\n",
        "                             (target_size[1] - image.height) // 2))\n",
        "        \n",
        "        return new_img\n",
        "    \n",
        "    def correct_perspective(self, image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Corrige la perspective si le plan est inclin√©\"\"\"\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
        "        \n",
        "        # D√©tection de contours\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        if len(contours) > 0:\n",
        "            # Plus grand contour (probablement le plan)\n",
        "            largest_contour = max(contours, key=cv2.contourArea)\n",
        "            \n",
        "            # Approximation polygonale\n",
        "            epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
        "            approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "            \n",
        "            if len(approx) == 4:\n",
        "                # Transformation de perspective\n",
        "                pts1 = np.float32(approx.reshape(4, 2))\n",
        "                pts2 = np.float32([[0, 0], [image.shape[1], 0],\n",
        "                                  [image.shape[1], image.shape[0]], [0, image.shape[0]]])\n",
        "                \n",
        "                M = cv2.getPerspectiveTransform(pts1, pts2)\n",
        "                corrected = cv2.warpPerspective(image, M, (image.shape[1], image.shape[0]))\n",
        "                return corrected\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def preprocess_pipeline(self, image: Image.Image, \n",
        "                           steps: List[str] = None) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Pipeline complet de preprocessing\n",
        "        \n",
        "        Args:\n",
        "            image: Image PIL √† traiter\n",
        "            steps: Liste des √©tapes ['orientation', 'binarize', 'denoise', 'normalize']\n",
        "        \"\"\"\n",
        "        if steps is None:\n",
        "            steps = ['orientation', 'binarize', 'denoise', 'normalize']\n",
        "        \n",
        "        img_array = np.array(image)\n",
        "        \n",
        "        # Correction orientation\n",
        "        if 'orientation' in steps:\n",
        "            image = self.correct_orientation(image)\n",
        "            img_array = np.array(image)\n",
        "        \n",
        "        # Binarisation\n",
        "        if 'binarize' in steps:\n",
        "            img_array = self.binarize_otsu(img_array)\n",
        "            # Conversion en RGB pour PIL\n",
        "            img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
        "        \n",
        "        # D√©noisage\n",
        "        if 'denoise' in steps:\n",
        "            if len(img_array.shape) == 3:\n",
        "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "            else:\n",
        "                gray = img_array\n",
        "            gray = self.denoise(gray, method='morphological')\n",
        "            img_array = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
        "        \n",
        "        # Normalisation taille\n",
        "        if 'normalize' in steps:\n",
        "            image = Image.fromarray(img_array)\n",
        "            image = self.normalize_size(image)\n",
        "        else:\n",
        "            image = Image.fromarray(img_array)\n",
        "        \n",
        "        return image\n",
        "\n",
        "# Initialisation\n",
        "preprocessor = ImagePreprocessor(config)\n",
        "\n",
        "# Fonction helper\n",
        "def preprocess_image(image_path: str, save_path: str = None) -> Image.Image:\n",
        "    \"\"\"Pr√©processe une image depuis un fichier\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    processed = preprocessor.preprocess_pipeline(image)\n",
        "    \n",
        "    if save_path:\n",
        "        processed.save(save_path)\n",
        "        print(f\"üíæ Image pr√©process√©e sauvegard√©e: {save_path}\")\n",
        "    \n",
        "    return processed\n",
        "\n",
        "print(\"‚úÖ Preprocesseur initialis√©!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Client OpenAI initialis√©\n",
            "üîÑ G√©n√©ration de sc√©narios avec LLM...\n",
            "‚ùå Erreur g√©n√©ration OpenAI: Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "‚úÖ 50 sc√©narios mock g√©n√©r√©s\n",
            "‚ö†Ô∏è Snowflake non disponible, sauvegarde locale\n",
            "üíæ Sc√©narios sauvegard√©s: data\\scenarios_llm.json\n",
            "\n",
            "üìä Exemple de sc√©nario g√©n√©r√©:\n",
            "{\n",
            "  \"scenario_id\": \"scenario_001\",\n",
            "  \"type_plan\": \"appartement\",\n",
            "  \"nombre_pieces\": 9,\n",
            "  \"superficie_totale_m2\": 97.55800549117619,\n",
            "  \"dimensions\": {\n",
            "    \"longueur_m\": 12.597383246960582,\n",
            "    \"largeur_m\": 12.277849324923924\n",
            "  },\n",
            "  \"elements\": [\n",
            "    {\n",
            "      \"type\": \"mur\",\n",
            "      \"position\": [\n",
            "        4.508341645004985,\n",
            "        7.335509628970192\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 1.506900340881258,\n",
            "        \"largeur_m\": 1.0508490765978518\n",
            "      },\n",
            "      \"orientation\": 180,\n",
            "      \"style\": \"contemporain\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"escalier\",\n",
            "      \"position\": [\n",
            "        4.856365481268828,\n",
            "        4.619686331087923\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 2.718061527471669,\n",
            "        \"largeur_m\": 1.126032447166044\n",
            "      },\n",
            "      \"orientation\": 270,\n",
            "      \"style\": \"moderne\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"escalier\",\n",
            "      \"position\": [\n",
            "        7.593414735178103,\n",
            "        7.800611591064361\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 0.7722459522818421,\n",
            "        \"largeur_m\": 1.1921001154372712\n",
            "      },\n",
            "      \"orientation\": 180,\n",
            "      \"style\": \"contemporain\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"porte\",\n",
            "      \"position\": [\n",
            "        11.099707223078203,\n",
            "        11.874523439026937\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 0.7682964432571655,\n",
            "        \"largeur_m\": 0.7660737298788582\n",
            "      },\n",
            "      \"orientation\": 90,\n",
            "      \"style\": \"moderne\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"fenetre\",\n",
            "      \"position\": [\n",
            "        12.260406281513738,\n",
            "        2.512969178121011\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 2.7355582508616276,\n",
            "        \"largeur_m\": 1.5140860590730074\n",
            "      },\n",
            "      \"orientation\": 270,\n",
            "      \"style\": \"contemporain\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"mur\",\n",
            "      \"position\": [\n",
            "        12.224829414785475,\n",
            "        8.33872590392841\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 2.522147622521406,\n",
            "        \"largeur_m\": 0.8061346180736484\n",
            "      },\n",
            "      \"orientation\": 270,\n",
            "      \"style\": \"classique\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"colonne\",\n",
            "      \"position\": [\n",
            "        6.989626197276671,\n",
            "        1.0788922491703248\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 1.6498099476580559,\n",
            "        \"largeur_m\": 0.9657912923386822\n",
            "      },\n",
            "      \"orientation\": 270,\n",
            "      \"style\": \"contemporain\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"colonne\",\n",
            "      \"position\": [\n",
            "        2.747703647302614,\n",
            "        8.017926166608197\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 1.2429377871325573,\n",
            "        \"largeur_m\": 1.7556655447965088\n",
            "      },\n",
            "      \"orientation\": 180,\n",
            "      \"style\": \"classique\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"mur\",\n",
            "      \"position\": [\n",
            "        2.5580284562828908,\n",
            "        0.29813594344223293\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 2.9636964697349266,\n",
            "        \"largeur_m\": 1.3276877108361105\n",
            "      },\n",
            "      \"orientation\": 180,\n",
            "      \"style\": \"classique\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"fenetre\",\n",
            "      \"position\": [\n",
            "        5.936012781521467,\n",
            "        11.809566415836821\n",
            "      ],\n",
            "      \"dimensions\": {\n",
            "        \"longueur_m\": 2.795636231466628,\n",
            "        \"largeur_m\": 1.7863406069808745\n",
            "      },\n",
            "      \"orientation\": 270,\n",
            "      \"style\": \"moderne\"\n",
            "    }\n",
            "  ],\n",
            "  \"pieces\": [],\n",
            "  \"style_architectural\": \"moderne\",\n",
            "  \"echelle\": \"1:50\",\n",
            "  \"orientation_batiment\": \"ouest\"\n",
            "}\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# G√âN√âRATION DE DONN√âES AVEC LLM\n",
        "# ============================================================================\n",
        "\n",
        "class LLMDataGenerator:\n",
        "    \"\"\"G√©n√©rateur de donn√©es synth√©tiques avec LLM\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.client = None\n",
        "        \n",
        "        if ANTHROPIC_AVAILABLE:\n",
        "            try:\n",
        "                self.client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\", \"\"))\n",
        "                print(\"‚úÖ Client Anthropic initialis√©\")\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è Anthropic non configur√©, utilisation mod√®le local\")\n",
        "    \n",
        "    def generate_scenarios_prompt(self, num_plans: int = 20) -> str:\n",
        "        \"\"\"\n",
        "        G√©n√®re le prompt pour cr√©er des sc√©narios de plans\n",
        "        \n",
        "        Args:\n",
        "            num_plans: Nombre de plans r√©els analys√©s\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "Tu es un expert en architecture. Analyse {num_plans} plans PDF d'architecte et g√©n√®re {self.config.NUM_SCENARIOS} sc√©narios JSON diff√©rents et vari√©s.\n",
        "\n",
        "Chaque sc√©nario doit repr√©senter un plan architectural r√©aliste avec les caract√©ristiques suivantes:\n",
        "\n",
        "STRUCTURE JSON REQUISE:\n",
        "{{\n",
        "    \"scenario_id\": \"unique_id\",\n",
        "    \"type_plan\": \"appartement\" | \"maison\" | \"bureau\" | \"commercial\" | \"industriel\",\n",
        "    \"nombre_pieces\": integer (2-15),\n",
        "    \"superficie_totale_m2\": float,\n",
        "    \"dimensions\": {{\n",
        "        \"longueur_m\": float,\n",
        "        \"largeur_m\": float,\n",
        "        \"hauteur_m\": float (optionnel)\n",
        "    }},\n",
        "    \"elements\": [\n",
        "        {{\n",
        "            \"type\": \"porte\" | \"fenetre\" | \"mur\" | \"escalier\" | \"colonne\" | \"sanitaire\" | \"cuisine\",\n",
        "            \"position\": [x_m, y_m],  # Coordonn√©es en m√®tres\n",
        "            \"dimensions\": {{\n",
        "                \"longueur_m\": float,\n",
        "                \"largeur_m\": float\n",
        "            }},\n",
        "            \"orientation\": 0 | 90 | 180 | 270,\n",
        "            \"style\": \"moderne\" | \"classique\" | \"contemporain\"\n",
        "        }},\n",
        "        ...\n",
        "    ],\n",
        "    \"pieces\": [\n",
        "        {{\n",
        "            \"nom\": \"salon\" | \"chambre\" | \"cuisine\" | \"salle_de_bain\" | etc.,\n",
        "            \"superficie_m2\": float,\n",
        "            \"position\": [x_m, y_m],\n",
        "            \"dimensions\": {{\"longueur_m\": float, \"largeur_m\": float}},\n",
        "            \"elements\": [\"porte_1\", \"fenetre_1\", ...]  # IDs des √©l√©ments\n",
        "        }},\n",
        "        ...\n",
        "    ],\n",
        "    \"style_architectural\": \"moderne\" | \"classique\" | \"contemporain\" | \"minimaliste\",\n",
        "    \"echelle\": \"1:100\" | \"1:50\" | \"1:200\",\n",
        "    \"orientation_batiment\": \"nord\" | \"sud\" | \"est\" | \"ouest\"\n",
        "}}\n",
        "\n",
        "CONTRAINTES:\n",
        "- Les coordonn√©es doivent √™tre coh√©rentes (pas d'√©l√©ments qui se chevauchent)\n",
        "- Les dimensions doivent √™tre r√©alistes (portes: 0.8-1.2m, fen√™tres: 0.6-2m)\n",
        "- Varier les types de plans (appartements, maisons, bureaux)\n",
        "- Inclure diff√©rents styles architecturaux\n",
        "- Assurer la coh√©rence spatiale\n",
        "\n",
        "G√©n√®re UNIQUEMENT un tableau JSON valide avec {self.config.NUM_SCENARIOS} objets, sans texte suppl√©mentaire.\n",
        "\"\"\"\n",
        "        return prompt\n",
        "    \n",
        "    def generate_scenarios_anthropic(self) -> List[Dict]:\n",
        "        \"\"\"G√©n√®re des sc√©narios avec Anthropic Claude\"\"\"\n",
        "        if not self.client:\n",
        "            print(\"‚ö†Ô∏è Anthropic non disponible, g√©n√©ration de sc√©narios mock\")\n",
        "            return self._generate_mock_scenarios()\n",
        "        \n",
        "        try:\n",
        "            prompt = self.generate_scenarios_prompt()\n",
        "            \n",
        "            # D√©terminer le mod√®le Anthropic (par d√©faut claude-3-5-sonnet-20241022)\n",
        "            model = self.config.LLM_MODEL if hasattr(self.config, 'LLM_MODEL') and 'claude' in self.config.LLM_MODEL.lower() else \"claude-3-5-sonnet-20241022\"\n",
        "            \n",
        "            response = self.client.messages.create(\n",
        "                model=model,\n",
        "                max_tokens=4096,\n",
        "                temperature=self.config.LLM_TEMPERATURE,\n",
        "                system=\"Tu es un expert en architecture et g√©n√©ration de donn√©es structur√©es. Tu g√©n√®res toujours du JSON valide.\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt + \"\\n\\nIMPORTANT: R√©ponds UNIQUEMENT avec un tableau JSON valide, sans texte suppl√©mentaire.\"}\n",
        "                ]\n",
        "            )\n",
        "            \n",
        "            content = response.content[0].text\n",
        "            \n",
        "            # Parsing JSON\n",
        "            try:\n",
        "                # Tentative d'extraction JSON depuis le texte\n",
        "                import re\n",
        "                # Chercher un tableau JSON\n",
        "                json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
        "                if json_match:\n",
        "                    data = json.loads(json_match.group())\n",
        "                    if isinstance(data, list):\n",
        "                        scenarios = data\n",
        "                    else:\n",
        "                        scenarios = [data]\n",
        "                else:\n",
        "                    # Essayer de parser directement\n",
        "                    data = json.loads(content)\n",
        "                    if isinstance(data, dict) and \"scenarios\" in data:\n",
        "                        scenarios = data[\"scenarios\"]\n",
        "                    elif isinstance(data, list):\n",
        "                        scenarios = data\n",
        "                    else:\n",
        "                        scenarios = [data]\n",
        "                \n",
        "                print(f\"‚úÖ {len(scenarios)} sc√©narios g√©n√©r√©s avec Anthropic\")\n",
        "                return scenarios[:self.config.NUM_SCENARIOS]\n",
        "                \n",
        "            except json.JSONDecodeError:\n",
        "                print(\"‚ö†Ô∏è Erreur parsing JSON, extraction manuelle...\")\n",
        "                # Tentative d'extraction JSON depuis le texte\n",
        "                import re\n",
        "                json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
        "                if json_match:\n",
        "                    scenarios = json.loads(json_match.group())\n",
        "                    return scenarios[:self.config.NUM_SCENARIOS]\n",
        "                return self._generate_mock_scenarios()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur g√©n√©ration Anthropic: {e}\")\n",
        "            return self._generate_mock_scenarios()\n",
        "    \n",
        "    def generate_scenarios_openai(self) -> List[Dict]:\n",
        "        \"\"\"Alias pour compatibilit√© - utilise Anthropic\"\"\"\n",
        "        return self.generate_scenarios_anthropic()\n",
        "    \n",
        "    def _generate_mock_scenarios(self, num: int = None) -> List[Dict]:\n",
        "        \"\"\"G√©n√®re des sc√©narios mock pour tests\"\"\"\n",
        "        if num is None:\n",
        "            num = self.config.NUM_SCENARIOS\n",
        "        \n",
        "        import random\n",
        "        scenarios = []\n",
        "        \n",
        "        types_plans = [\"appartement\", \"maison\", \"bureau\", \"commercial\"]\n",
        "        types_elements = [\"porte\", \"fenetre\", \"mur\", \"escalier\", \"colonne\", \"sanitaire\"]\n",
        "        styles = [\"moderne\", \"classique\", \"contemporain\"]\n",
        "        \n",
        "        for i in range(num):\n",
        "            type_plan = random.choice(types_plans)\n",
        "            nb_pieces = random.randint(2, 10)\n",
        "            \n",
        "            scenario = {\n",
        "                \"scenario_id\": f\"scenario_{i+1:03d}\",\n",
        "                \"type_plan\": type_plan,\n",
        "                \"nombre_pieces\": nb_pieces,\n",
        "                \"superficie_totale_m2\": random.uniform(30, 200),\n",
        "                \"dimensions\": {\n",
        "                    \"longueur_m\": random.uniform(5, 20),\n",
        "                    \"largeur_m\": random.uniform(4, 15)\n",
        "                },\n",
        "                \"elements\": [],\n",
        "                \"pieces\": [],\n",
        "                \"style_architectural\": random.choice(styles),\n",
        "                \"echelle\": random.choice([\"1:100\", \"1:50\", \"1:200\"]),\n",
        "                \"orientation_batiment\": random.choice([\"nord\", \"sud\", \"est\", \"ouest\"])\n",
        "            }\n",
        "            \n",
        "            # G√©n√©ration √©l√©ments\n",
        "            nb_elements = random.randint(5, 20)\n",
        "            for j in range(nb_elements):\n",
        "                elem_type = random.choice(types_elements)\n",
        "                scenario[\"elements\"].append({\n",
        "                    \"type\": elem_type,\n",
        "                    \"position\": [\n",
        "                        random.uniform(0, scenario[\"dimensions\"][\"longueur_m\"]),\n",
        "                        random.uniform(0, scenario[\"dimensions\"][\"largeur_m\"])\n",
        "                    ],\n",
        "                    \"dimensions\": {\n",
        "                        \"longueur_m\": random.uniform(0.5, 3.0),\n",
        "                        \"largeur_m\": random.uniform(0.5, 2.0)\n",
        "                    },\n",
        "                    \"orientation\": random.choice([0, 90, 180, 270]),\n",
        "                    \"style\": random.choice(styles)\n",
        "                })\n",
        "            \n",
        "            scenarios.append(scenario)\n",
        "        \n",
        "        print(f\"‚úÖ {len(scenarios)} sc√©narios mock g√©n√©r√©s\")\n",
        "        return scenarios\n",
        "    \n",
        "    def save_scenarios_snowflake(self, scenarios: List[Dict]):\n",
        "        \"\"\"Sauvegarde les sc√©narios dans Snowflake\"\"\"\n",
        "        if not snowflake_connected:\n",
        "            print(\"‚ö†Ô∏è Snowflake non disponible, sauvegarde locale\")\n",
        "            # Sauvegarde locale\n",
        "            scenarios_path = config.DATA_DIR / \"scenarios_llm.json\"\n",
        "            with open(scenarios_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(scenarios, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"üíæ Sc√©narios sauvegard√©s: {scenarios_path}\")\n",
        "            return\n",
        "        \n",
        "        for scenario in scenarios:\n",
        "            query = f\"\"\"\n",
        "            INSERT INTO scenarios_llm \n",
        "            (scenario_id, parametres_json, type_plan, nombre_elements)\n",
        "            VALUES (\n",
        "                '{scenario['scenario_id']}',\n",
        "                PARSE_JSON('{json.dumps(scenario)}'),\n",
        "                '{scenario['type_plan']}',\n",
        "                {len(scenario.get('elements', []))}\n",
        "            )\n",
        "            \"\"\"\n",
        "            snowflake_mgr.execute_query(query, fetch=False)\n",
        "        \n",
        "        print(f\"‚úÖ {len(scenarios)} sc√©narios sauvegard√©s dans Snowflake\")\n",
        "\n",
        "# Initialisation\n",
        "llm_generator = LLMDataGenerator(config)\n",
        "\n",
        "# G√©n√©ration des sc√©narios\n",
        "print(\"üîÑ G√©n√©ration de sc√©narios avec LLM...\")\n",
        "scenarios = llm_generator.generate_scenarios_openai()\n",
        "\n",
        "# Sauvegarde\n",
        "llm_generator.save_scenarios_snowflake(scenarios)\n",
        "\n",
        "print(f\"\\nüìä Exemple de sc√©nario g√©n√©r√©:\")\n",
        "if scenarios:\n",
        "    print(json.dumps(scenarios[0], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8408b824e8c94a76a9392d32e639bdcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30f7562c1df04e22a6c5062a77b63bbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "diffusion_pytorch_model.safetensors:  69%|######8   | 2.36G/3.44G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc44462e637f4ff6a72d539fe0f991fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# G√âN√âRATION D'IMAGES SYNTH√âTIQUES\n",
        "# ============================================================================\n",
        "\n",
        "class SyntheticImageGenerator:\n",
        "    \"\"\"G√©n√©rateur d'images synth√©tiques de plans avec IA g√©n√©rative\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.diffusion_pipeline = None\n",
        "        \n",
        "        # Tentative de chargement Stable Diffusion\n",
        "        try:\n",
        "            from diffusers import StableDiffusionPipeline\n",
        "            import torch\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                self.diffusion_pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                    \"runwayml/stable-diffusion-v1-5\",\n",
        "                    torch_dtype=torch.float16\n",
        "                ).to(\"cuda\")\n",
        "            else:\n",
        "                self.diffusion_pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                    \"runwayml/stable-diffusion-v1-5\"\n",
        "                )\n",
        "            print(\"‚úÖ Pipeline Stable Diffusion charg√©\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Stable Diffusion non disponible: {e}\")\n",
        "            print(\"üí° Utilisation de g√©n√©ration proc√©durale\")\n",
        "    \n",
        "    def scenario_to_prompt(self, scenario: Dict) -> str:\n",
        "        \"\"\"Convertit un sc√©nario JSON en prompt pour g√©n√©ration d'image\"\"\"\n",
        "        type_plan = scenario.get('type_plan', 'appartement')\n",
        "        style = scenario.get('style_architectural', 'moderne')\n",
        "        nb_pieces = scenario.get('nombre_pieces', 3)\n",
        "        \n",
        "        elements_desc = []\n",
        "        for elem in scenario.get('elements', [])[:10]:  # Limiter pour le prompt\n",
        "            elements_desc.append(elem['type'])\n",
        "        \n",
        "        prompt = f\"\"\"\n",
        "Architectural floor plan, {type_plan}, {style} style, {nb_pieces} rooms,\n",
        "elements: {', '.join(set(elements_desc))},\n",
        "technical drawing, black and white, clean lines, 2D top view,\n",
        "high quality, detailed, professional architecture blueprint\n",
        "\"\"\"\n",
        "        return prompt.strip()\n",
        "    \n",
        "    def generate_image_stable_diffusion(self, prompt: str, \n",
        "                                       num_inference_steps: int = 50) -> Image.Image:\n",
        "        \"\"\"G√©n√®re une image avec Stable Diffusion\"\"\"\n",
        "        if not self.diffusion_pipeline:\n",
        "            return self.generate_image_procedural(prompt)\n",
        "        \n",
        "        try:\n",
        "            negative_prompt = \"color, 3D, perspective, photo, realistic, messy, low quality\"\n",
        "            \n",
        "            image = self.diffusion_pipeline(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=7.5,\n",
        "                height=512,\n",
        "                width=512\n",
        "            ).images[0]\n",
        "            \n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur g√©n√©ration Stable Diffusion: {e}\")\n",
        "            return self.generate_image_procedural(prompt)\n",
        "    \n",
        "    def generate_image_procedural(self, scenario: Dict) -> Image.Image:\n",
        "        \"\"\"\n",
        "        G√©n√®re une image proc√©durale bas√©e sur le sc√©nario JSON\n",
        "        Alternative si Stable Diffusion n'est pas disponible\n",
        "        \"\"\"\n",
        "        width, height = 1024, 1024\n",
        "        img = Image.new('RGB', (width, height), color='white')\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        \n",
        "        # Dimensions du plan\n",
        "        plan_w = scenario['dimensions']['longueur_m']\n",
        "        plan_h = scenario['dimensions']['largeur_m']\n",
        "        \n",
        "        # √âchelle pour le dessin\n",
        "        scale_x = (width - 100) / plan_w\n",
        "        scale_y = (height - 100) / plan_h\n",
        "        scale = min(scale_x, scale_y)\n",
        "        \n",
        "        offset_x = (width - plan_w * scale) / 2\n",
        "        offset_y = (height - plan_h * scale) / 2\n",
        "        \n",
        "        # Dessin des murs (contour)\n",
        "        wall_thickness = 20\n",
        "        draw.rectangle(\n",
        "            [offset_x, offset_y, \n",
        "             offset_x + plan_w * scale, offset_y + plan_h * scale],\n",
        "            outline='black', width=wall_thickness\n",
        "        )\n",
        "        \n",
        "        # Dessin des √©l√©ments\n",
        "        for elem in scenario.get('elements', []):\n",
        "            elem_type = elem['type']\n",
        "            pos = elem['position']\n",
        "            dims = elem['dimensions']\n",
        "            \n",
        "            x = offset_x + pos[0] * scale\n",
        "            y = offset_y + pos[1] * scale\n",
        "            w = dims['longueur_m'] * scale\n",
        "            h = dims['largeur_m'] * scale\n",
        "            \n",
        "            if elem_type == 'porte':\n",
        "                # Porte: rectangle avec arc\n",
        "                draw.rectangle([x, y, x + w, y + h], outline='black', width=3)\n",
        "                # Arc de cercle pour l'ouverture\n",
        "                draw.arc([x - w/2, y, x + w/2, y + h], start=0, end=180, \n",
        "                        fill='black', width=2)\n",
        "            elif elem_type == 'fenetre':\n",
        "                # Fen√™tre: rectangle double ligne\n",
        "                draw.rectangle([x, y, x + w, y + h], outline='black', width=3)\n",
        "                draw.rectangle([x + 5, y + 5, x + w - 5, y + h - 5], \n",
        "                             outline='black', width=2)\n",
        "            elif elem_type == 'mur':\n",
        "                # Mur: ligne √©paisse\n",
        "                draw.rectangle([x, y, x + w, y + h], fill='black')\n",
        "            elif elem_type == 'escalier':\n",
        "                # Escalier: rectangles en escalier\n",
        "                for i in range(5):\n",
        "                    draw.rectangle([x + i*10, y + i*10, x + w, y + h/5 + i*10],\n",
        "                                 outline='black', width=2)\n",
        "            else:\n",
        "                # Autres √©l√©ments: rectangle simple\n",
        "                draw.rectangle([x, y, x + w, y + h], outline='black', width=2)\n",
        "        \n",
        "        return img\n",
        "    \n",
        "    def generate_images_from_scenarios(self, scenarios: List[Dict], \n",
        "                                      output_dir: Path) -> List[str]:\n",
        "        \"\"\"G√©n√®re des images pour tous les sc√©narios\"\"\"\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        generated_paths = []\n",
        "        \n",
        "        for scenario in scenarios:\n",
        "            scenario_id = scenario['scenario_id']\n",
        "            \n",
        "            # G√©n√©ration\n",
        "            if self.diffusion_pipeline:\n",
        "                prompt = self.scenario_to_prompt(scenario)\n",
        "                image = self.generate_image_stable_diffusion(prompt)\n",
        "            else:\n",
        "                image = self.generate_image_procedural(scenario)\n",
        "            \n",
        "            # Sauvegarde\n",
        "            img_path = output_dir / f\"{scenario_id}.png\"\n",
        "            image.save(img_path, \"PNG\")\n",
        "            generated_paths.append(str(img_path))\n",
        "            \n",
        "            print(f\"  ‚úÖ Image g√©n√©r√©e: {scenario_id}\")\n",
        "        \n",
        "        print(f\"‚úÖ {len(generated_paths)} images synth√©tiques g√©n√©r√©es\")\n",
        "        return generated_paths\n",
        "\n",
        "# Initialisation\n",
        "image_generator = SyntheticImageGenerator(config)\n",
        "\n",
        "# G√©n√©ration d'images (exemple avec les 5 premiers sc√©narios)\n",
        "if scenarios:\n",
        "    print(\"üîÑ G√©n√©ration d'images synth√©tiques...\")\n",
        "    synthetic_images_dir = config.DATA_DIR / \"synthetic_images\"\n",
        "    generated_paths = image_generator.generate_images_from_scenarios(\n",
        "        scenarios[:5],  # Limiter pour la d√©mo\n",
        "        synthetic_images_dir\n",
        "    )\n",
        "    \n",
        "    # Affichage d'un exemple\n",
        "    if generated_paths:\n",
        "        example_img = Image.open(generated_paths[0])\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(example_img)\n",
        "        plt.title(f\"Exemple d'image synth√©tique: {Path(generated_paths[0]).stem}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AUGMENTATION DE DONN√âES\n",
        "# ============================================================================\n",
        "\n",
        "class DataAugmenter:\n",
        "    \"\"\"Augmente les donn√©es d'entra√Ænement\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    \n",
        "    def rotate_image(self, image: Image.Image, angle: int) -> Image.Image:\n",
        "        \"\"\"Rotation de l'image\"\"\"\n",
        "        return image.rotate(angle, expand=True, fillcolor='white')\n",
        "    \n",
        "    def flip_image(self, image: Image.Image, direction: str = 'horizontal') -> Image.Image:\n",
        "        \"\"\"Retournement horizontal ou vertical\"\"\"\n",
        "        if direction == 'horizontal':\n",
        "            return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        elif direction == 'vertical':\n",
        "            return image.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "        return image\n",
        "    \n",
        "    def scale_image(self, image: Image.Image, scale_factor: float) -> Image.Image:\n",
        "        \"\"\"Redimensionnement avec facteur d'√©chelle\"\"\"\n",
        "        new_size = (int(image.width * scale_factor), int(image.height * scale_factor))\n",
        "        return image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "    \n",
        "    def add_noise(self, image: np.ndarray, noise_level: float = 0.1) -> np.ndarray:\n",
        "        \"\"\"Ajoute du bruit gaussien\"\"\"\n",
        "        noise = np.random.normal(0, noise_level * 255, image.shape).astype(np.uint8)\n",
        "        noisy = cv2.add(image.astype(np.uint8), noise)\n",
        "        return np.clip(noisy, 0, 255)\n",
        "    \n",
        "    def adjust_brightness(self, image: Image.Image, factor: float) -> Image.Image:\n",
        "        \"\"\"Ajuste la luminosit√©\"\"\"\n",
        "        from PIL import ImageEnhance\n",
        "        enhancer = ImageEnhance.Brightness(image)\n",
        "        return enhancer.enhance(factor)\n",
        "    \n",
        "    def augment_image(self, image: Image.Image, \n",
        "                     augmentations: List[str] = None) -> List[Image.Image]:\n",
        "        \"\"\"\n",
        "        Applique plusieurs augmentations √† une image\n",
        "        \n",
        "        Args:\n",
        "            image: Image √† augmenter\n",
        "            augmentations: Liste des types d'augmentation √† appliquer\n",
        "            \n",
        "        Returns:\n",
        "            Liste d'images augment√©es\n",
        "        \"\"\"\n",
        "        if augmentations is None:\n",
        "            augmentations = ['rotate', 'flip', 'scale', 'noise', 'brightness']\n",
        "        \n",
        "        augmented_images = []\n",
        "        img_array = np.array(image)\n",
        "        \n",
        "        # Rotations\n",
        "        if 'rotate' in augmentations:\n",
        "            for angle in self.config.AUGMENTATION_ROTATIONS:\n",
        "                rotated = self.rotate_image(image, angle)\n",
        "                augmented_images.append(rotated)\n",
        "        \n",
        "        # Retournements\n",
        "        if 'flip' in augmentations and self.config.AUGMENTATION_FLIP:\n",
        "            flipped_h = self.flip_image(image, 'horizontal')\n",
        "            flipped_v = self.flip_image(image, 'vertical')\n",
        "            augmented_images.extend([flipped_h, flipped_v])\n",
        "        \n",
        "        # Redimensionnements\n",
        "        if 'scale' in augmentations:\n",
        "            for scale in self.config.AUGMENTATION_SCALE:\n",
        "                if scale != 1.0:\n",
        "                    scaled = self.scale_image(image, scale)\n",
        "                    # Recadrer √† la taille originale\n",
        "                    scaled = scaled.crop((0, 0, image.width, image.height))\n",
        "                    augmented_images.append(scaled)\n",
        "        \n",
        "        # Bruit\n",
        "        if 'noise' in augmentations and self.config.AUGMENTATION_NOISE:\n",
        "            noisy = self.add_noise(img_array, noise_level=0.05)\n",
        "            augmented_images.append(Image.fromarray(noisy))\n",
        "        \n",
        "        # Luminosit√©\n",
        "        if 'brightness' in augmentations:\n",
        "            bright = self.adjust_brightness(image, 1.2)\n",
        "            dark = self.adjust_brightness(image, 0.8)\n",
        "            augmented_images.extend([bright, dark])\n",
        "        \n",
        "        return augmented_images\n",
        "    \n",
        "    def augment_dataset(self, image_paths: List[str], \n",
        "                       output_dir: Path, prefix: str = \"aug\") -> List[str]:\n",
        "        \"\"\"\n",
        "        Augmente un dataset complet\n",
        "        \n",
        "        Args:\n",
        "            image_paths: Liste des chemins d'images\n",
        "            output_dir: R√©pertoire de sortie\n",
        "            prefix: Pr√©fixe pour les images augment√©es\n",
        "            \n",
        "        Returns:\n",
        "            Liste des chemins des images augment√©es\n",
        "        \"\"\"\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        augmented_paths = []\n",
        "        \n",
        "        for img_path in image_paths:\n",
        "            image = Image.open(img_path)\n",
        "            base_name = Path(img_path).stem\n",
        "            \n",
        "            # Augmentations\n",
        "            augmented = self.augment_image(image)\n",
        "            \n",
        "            # Sauvegarde\n",
        "            for idx, aug_img in enumerate(augmented):\n",
        "                aug_path = output_dir / f\"{prefix}_{base_name}_{idx:03d}.png\"\n",
        "                aug_img.save(aug_path, \"PNG\")\n",
        "                augmented_paths.append(str(aug_path))\n",
        "            \n",
        "            print(f\"  ‚úÖ {len(augmented)} augmentations pour {base_name}\")\n",
        "        \n",
        "        print(f\"‚úÖ {len(augmented_paths)} images augment√©es g√©n√©r√©es\")\n",
        "        return augmented_paths\n",
        "\n",
        "# Initialisation\n",
        "augmenter = DataAugmenter(config)\n",
        "\n",
        "# Exemple d'augmentation (si vous avez des images)\n",
        "print(\"üìù Pour augmenter vos images:\")\n",
        "print(\"augmented_paths = augmenter.augment_dataset(image_paths, output_dir)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LABELLISATION AUTOMATIQUE AVEC YOLO\n",
        "# ============================================================================\n",
        "#\n",
        "# ‚ö†Ô∏è IMPORTANT: YOLO pr√©-entra√Æn√© d√©tecte des objets g√©n√©riques (COCO dataset)\n",
        "#    - Classes d√©tect√©es: person, car, bicycle, chair, etc. (80 classes COCO)\n",
        "#    - PAS d'√©l√©ments architecturaux sp√©cifiques (porte, fen√™tre, mur, etc.)\n",
        "#\n",
        "# Pour d√©tecter des √©l√©ments architecturaux:\n",
        "#    1. Fine-tuner YOLO sur un dataset de plans architecturaux (Section 6)\n",
        "#    2. Utiliser un mod√®le pr√©-entra√Æn√© sp√©cialis√© en architecture\n",
        "#    3. Utiliser un mod√®le de segmentation s√©mantique sp√©cialis√©\n",
        "#\n",
        "# Le code ci-dessous utilise les noms de classes r√©els de YOLO (result.names)\n",
        "# au lieu de config.CLASSES qui est pour un mod√®le fine-tun√©.\n",
        "# ============================================================================\n",
        "\n",
        "class YOLOLabeler:\n",
        "    \"\"\"Labellisateur automatique avec YOLO pr√©-entra√Æn√©\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.model_detection = None\n",
        "        self.model_segmentation = None\n",
        "        self.model_keypoints = None\n",
        "        \n",
        "        self._load_models()\n",
        "    \n",
        "    def _load_models(self):\n",
        "        \"\"\"Charge les mod√®les YOLO pr√©-entra√Æn√©s\"\"\"\n",
        "        try:\n",
        "            # Mod√®le d√©tection\n",
        "            self.model_detection = YOLO(self.config.YOLO_MODEL_DETECTION)\n",
        "            print(f\"‚úÖ Mod√®le d√©tection charg√©: {self.config.YOLO_MODEL_DETECTION}\")\n",
        "            \n",
        "            # Mod√®le segmentation\n",
        "            self.model_segmentation = YOLO(self.config.YOLO_MODEL_SEGMENTATION)\n",
        "            print(f\"‚úÖ Mod√®le segmentation charg√©: {self.config.YOLO_MODEL_SEGMENTATION}\")\n",
        "            \n",
        "            # Mod√®le keypoints\n",
        "            self.model_keypoints = YOLO(self.config.YOLO_MODEL_KEYPOINTS)\n",
        "            print(f\"‚úÖ Mod√®le keypoints charg√©: {self.config.YOLO_MODEL_KEYPOINTS}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur chargement mod√®les YOLO: {e}\")\n",
        "            print(\"üí° T√©l√©chargez les mod√®les depuis: https://github.com/ultralytics/ultralytics\")\n",
        "    \n",
        "    def predict_detection(self, image_path: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Pr√©dit les d√©tections sur une image\n",
        "        \n",
        "        Returns:\n",
        "            Liste de d√©tections avec bbox, confidence, classe\n",
        "        \"\"\"\n",
        "        if not self.model_detection:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            results = self.model_detection.predict(\n",
        "                image_path,\n",
        "                imgsz=self.config.YOLO_IMG_SIZE,\n",
        "                conf=self.config.YOLO_CONF_THRESHOLD,\n",
        "                iou=self.config.YOLO_IOU_THRESHOLD,\n",
        "                verbose=False\n",
        "            )\n",
        "            \n",
        "            detections = []\n",
        "            for result in results:\n",
        "                boxes = result.boxes\n",
        "                # Utiliser les noms de classes r√©els de YOLO (COCO dataset)\n",
        "                # YOLO d√©tecte des objets g√©n√©riques, pas des √©l√©ments architecturaux sp√©cifiques\n",
        "                # Pour d√©tecter des √©l√©ments architecturaux, il faudrait un mod√®le fine-tun√©\n",
        "                for box in boxes:\n",
        "                    # Coordonn√©es normalis√©es YOLO (x_center, y_center, width, height)\n",
        "                    x_center, y_center, width, height = box.xywhn[0].cpu().numpy()\n",
        "                    conf = float(box.conf[0].cpu().numpy())\n",
        "                    cls = int(box.cls[0].cpu().numpy())\n",
        "                    \n",
        "                    # Utiliser les noms de classes r√©els de YOLO (result.names)\n",
        "                    # Au lieu de config.CLASSES qui est pour un mod√®le fine-tun√©\n",
        "                    class_name = result.names[cls] if hasattr(result, 'names') and cls in result.names else f\"class_{cls}\"\n",
        "                    \n",
        "                    detections.append({\n",
        "                        'class': cls,\n",
        "                        'class_name': class_name,\n",
        "                        'confidence': conf,\n",
        "                        'bbox': {\n",
        "                            'x_center': float(x_center),\n",
        "                            'y_center': float(y_center),\n",
        "                            'width': float(width),\n",
        "                            'height': float(height)\n",
        "                        }\n",
        "                    })\n",
        "            \n",
        "            return detections\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur pr√©diction d√©tection: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def predict_segmentation(self, image_path: str) -> List[Dict]:\n",
        "        \"\"\"Pr√©dit la segmentation s√©mantique\"\"\"\n",
        "        if not self.model_segmentation:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            results = self.model_segmentation.predict(\n",
        "                image_path,\n",
        "                imgsz=self.config.YOLO_IMG_SIZE,\n",
        "                conf=self.config.YOLO_CONF_THRESHOLD,\n",
        "                verbose=False\n",
        "            )\n",
        "            \n",
        "            segmentations = []\n",
        "            for result in results:\n",
        "                if result.masks is not None:\n",
        "                    for mask, box in zip(result.masks, result.boxes):\n",
        "                        # Points de segmentation\n",
        "                        seg_points = mask.xy[0].cpu().numpy()\n",
        "                        cls = int(box.cls[0].cpu().numpy())\n",
        "                        conf = float(box.conf[0].cpu().numpy())\n",
        "                        \n",
        "                        # Utiliser les noms de classes r√©els de YOLO\n",
        "                        class_name = result.names[cls] if hasattr(result, 'names') and cls in result.names else f\"class_{cls}\"\n",
        "                        segmentations.append({\n",
        "                            'class': cls,\n",
        "                            'class_name': class_name,\n",
        "                            'confidence': conf,\n",
        "                            'segmentation_points': seg_points.tolist()\n",
        "                        })\n",
        "            \n",
        "            return segmentations\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur pr√©diction segmentation: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def predict_keypoints(self, image_path: str) -> List[Dict]:\n",
        "        \"\"\"Pr√©dit les keypoints\"\"\"\n",
        "        if not self.model_keypoints:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            results = self.model_keypoints.predict(\n",
        "                image_path,\n",
        "                imgsz=self.config.YOLO_IMG_SIZE,\n",
        "                conf=self.config.YOLO_CONF_THRESHOLD,\n",
        "                verbose=False\n",
        "            )\n",
        "            \n",
        "            keypoints_list = []\n",
        "            for result in results:\n",
        "                if result.keypoints is not None:\n",
        "                    for kpts, box in zip(result.keypoints, result.boxes):\n",
        "                        kpts_array = kpts.xy[0].cpu().numpy()\n",
        "                        cls = int(box.cls[0].cpu().numpy())\n",
        "                        conf = float(box.conf[0].cpu().numpy())\n",
        "                        \n",
        "                        keypoints_list.append({\n",
        "                            'class': cls,\n",
        "                            'confidence': conf,\n",
        "                            'keypoints': kpts_array.tolist()\n",
        "                        })\n",
        "            \n",
        "            return keypoints_list\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur pr√©diction keypoints: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def export_yolo_format(self, detections: List[Dict], \n",
        "                          image_path: str, output_dir: Path) -> str:\n",
        "        \"\"\"\n",
        "        Exporte les annotations au format YOLO\n",
        "        \n",
        "        Format YOLO: class_id x_center y_center width height (toutes valeurs normalis√©es 0-1)\n",
        "        \"\"\"\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Fichier .txt avec le m√™me nom que l'image\n",
        "        txt_path = output_dir / f\"{Path(image_path).stem}.txt\"\n",
        "        \n",
        "        with open(txt_path, 'w') as f:\n",
        "            for det in detections:\n",
        "                bbox = det['bbox']\n",
        "                line = f\"{det['class']} {bbox['x_center']} {bbox['y_center']} {bbox['width']} {bbox['height']}\\n\"\n",
        "                f.write(line)\n",
        "        \n",
        "        return str(txt_path)\n",
        "    \n",
        "    def label_image(self, image_path: str, save_annotations: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Labellise compl√®tement une image (d√©tection + segmentation + keypoints)\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec toutes les annotations\n",
        "        \"\"\"\n",
        "        detections = self.predict_detection(image_path)\n",
        "        segmentations = self.predict_segmentation(image_path)\n",
        "        keypoints = self.predict_keypoints(image_path)\n",
        "        \n",
        "        annotations = {\n",
        "            'image_path': image_path,\n",
        "            'detections': detections,\n",
        "            'segmentations': segmentations,\n",
        "            'keypoints': keypoints,\n",
        "            'num_detections': len(detections)\n",
        "        }\n",
        "        \n",
        "        # Export format YOLO\n",
        "        if save_annotations and detections:\n",
        "            self.export_yolo_format(\n",
        "                detections, \n",
        "                image_path, \n",
        "                config.ANNOTATIONS_DIR\n",
        "            )\n",
        "        \n",
        "        return annotations\n",
        "    \n",
        "    def visualize_annotations(self, image_path: str, annotations: Dict) -> Image.Image:\n",
        "        \"\"\"Visualise les annotations sur l'image\"\"\"\n",
        "        image = Image.open(image_path)\n",
        "        img_array = np.array(image)\n",
        "        \n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "        ax.imshow(img_array)\n",
        "        \n",
        "        # Dessin des bboxes\n",
        "        for det in annotations['detections']:\n",
        "            bbox = det['bbox']\n",
        "            img_h, img_w = img_array.shape[:2]\n",
        "            \n",
        "            # Conversion coordonn√©es normalis√©es ‚Üí pixels\n",
        "            x_center = bbox['x_center'] * img_w\n",
        "            y_center = bbox['y_center'] * img_h\n",
        "            width = bbox['width'] * img_w\n",
        "            height = bbox['height'] * img_h\n",
        "            \n",
        "            x1 = x_center - width / 2\n",
        "            y1 = y_center - height / 2\n",
        "            \n",
        "            rect = patches.Rectangle(\n",
        "                (x1, y1), width, height,\n",
        "                linewidth=2, edgecolor='red', facecolor='none'\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "            \n",
        "            # Label\n",
        "            ax.text(x1, y1 - 5, \n",
        "                   f\"{det['class_name']} {det['confidence']:.2f}\",\n",
        "                   color='red', fontsize=10, weight='bold')\n",
        "        \n",
        "        ax.set_title(f\"Annotations - {Path(image_path).name}\")\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "# Initialisation\n",
        "labeler = YOLOLabeler(config)\n",
        "\n",
        "print(\"‚úÖ Labellisateur YOLO initialis√©!\")\n",
        "print(\"üí° Pour labelliser une image: annotations = labeler.label_image('path/to/image.png')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STOCKAGE ANNOTATIONS SNOWFLAKE\n",
        "# ============================================================================\n",
        "\n",
        "def save_annotations_snowflake(plan_id: str, annotations: Dict):\n",
        "    \"\"\"Sauvegarde les annotations dans Snowflake\"\"\"\n",
        "    if not snowflake_connected:\n",
        "        print(\"‚ö†Ô∏è Snowflake non disponible, sauvegarde locale\")\n",
        "        # Sauvegarde locale JSON\n",
        "        json_path = config.ANNOTATIONS_DIR / f\"{plan_id}_annotations.json\"\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(annotations, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"üíæ Annotations sauvegard√©es: {json_path}\")\n",
        "        return\n",
        "    \n",
        "    # Insertion des d√©tections\n",
        "    for det in annotations['detections']:\n",
        "        query = f\"\"\"\n",
        "        INSERT INTO annotations \n",
        "        (id, plan_id, classe, bbox_x, bbox_y, bbox_width, bbox_height, confidence)\n",
        "        VALUES (\n",
        "            '{plan_id}_det_{len(annotations['detections'])}',\n",
        "            '{plan_id}',\n",
        "            '{det['class_name']}',\n",
        "            {det['bbox']['x_center']},\n",
        "            {det['bbox']['y_center']},\n",
        "            {det['bbox']['width']},\n",
        "            {det['bbox']['height']},\n",
        "            {det['confidence']}\n",
        "        )\n",
        "        \"\"\"\n",
        "        snowflake_mgr.execute_query(query, fetch=False)\n",
        "    \n",
        "    # Insertion des segmentations\n",
        "    for seg in annotations['segmentations']:\n",
        "        query = f\"\"\"\n",
        "        INSERT INTO annotations \n",
        "        (id, plan_id, classe, segmentation_points, confidence)\n",
        "        VALUES (\n",
        "            '{plan_id}_seg_{len(annotations['segmentations'])}',\n",
        "            '{plan_id}',\n",
        "            '{seg['class_name']}',\n",
        "            PARSE_JSON('{json.dumps(seg['segmentation_points'])}'),\n",
        "            {seg['confidence']}\n",
        "        )\n",
        "        \"\"\"\n",
        "        snowflake_mgr.execute_query(query, fetch=False)\n",
        "    \n",
        "    print(f\"‚úÖ {len(annotations['detections'])} annotations sauvegard√©es dans Snowflake\")\n",
        "\n",
        "# Exemple d'utilisation\n",
        "print(\"üìù Fonction de sauvegarde annotations pr√™te\")\n",
        "print(\"üí° Utilisation: save_annotations_snowflake(plan_id, annotations)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 6: Entra√Ænement YOLO\n",
        "\n",
        "## 6.1 Pr√©paration du dataset (train/val/test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 10: Exemples d'Utilisation Pratique\n",
        "\n",
        "## üéØ Exemples pour D√©marrer Rapidement\n",
        "\n",
        "Cette section contient des exemples pr√™ts √† l'emploi pour utiliser le projet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.1 Exemple Minimal: Traiter un PDF et D√©tecter les √âl√©ments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXEMPLE 1: TRAITEMENT COMPLET D'UN PDF\n",
        "# ============================================================================\n",
        "\n",
        "def exemple_traitement_pdf_complet(pdf_path: str):\n",
        "    \"\"\"\n",
        "    Exemple complet: Extraction ‚Üí Preprocessing ‚Üí D√©tection ‚Üí Visualisation\n",
        "    \n",
        "    Usage:\n",
        "        exemple_traitement_pdf_complet(\"data/pdfs/mon_plan.pdf\")\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ TRAITEMENT COMPLET D'UN PDF\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # √âtape 1: Extraction PDF ‚Üí Images\n",
        "    print(\"\\nüìÑ √âtape 1: Extraction des pages PDF...\")\n",
        "    result = process_pdf(pdf_path)\n",
        "    \n",
        "    if not result:\n",
        "        print(\"‚ùå Erreur lors de l'extraction\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"‚úÖ {len(result['images'])} pages extraites\")\n",
        "    \n",
        "    # √âtape 2: Preprocessing (premi√®re page)\n",
        "    print(\"\\nüîß √âtape 2: Preprocessing de la premi√®re page...\")\n",
        "    first_image_path = result['saved_paths'][0]\n",
        "    processed_img = preprocess_image(\n",
        "        first_image_path,\n",
        "        save_path=str(config.IMAGES_DIR / f\"processed_{Path(first_image_path).name}\")\n",
        "    )\n",
        "    print(\"‚úÖ Image pr√©process√©e\")\n",
        "    \n",
        "    # √âtape 3: D√©tection avec YOLO\n",
        "    print(\"\\nüîç √âtape 3: D√©tection des √©l√©ments architecturaux...\")\n",
        "    annotations = labeler.label_image(first_image_path)\n",
        "    \n",
        "    print(f\"‚úÖ {len(annotations['detections'])} √©l√©ments d√©tect√©s:\")\n",
        "    for det in annotations['detections']:\n",
        "        print(f\"   - {det['class_name']}: confiance {det['confidence']:.2%}\")\n",
        "    \n",
        "    # √âtape 4: Visualisation\n",
        "    print(\"\\nüìä √âtape 4: Visualisation des r√©sultats...\")\n",
        "    fig = labeler.visualize_annotations(first_image_path, annotations)\n",
        "    plt.show()\n",
        "    \n",
        "    # √âtape 5: Sauvegarde (optionnel)\n",
        "    if snowflake_connected:\n",
        "        print(\"\\nüíæ √âtape 5: Sauvegarde dans Snowflake...\")\n",
        "        save_annotations_snowflake(result['plan_data']['id'], annotations)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ TRAITEMENT TERMIN√â AVEC SUCC√àS!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return {\n",
        "        'result': result,\n",
        "        'annotations': annotations,\n",
        "        'processed_image': processed_img\n",
        "    }\n",
        "\n",
        "# D√âCOMMENTER POUR UTILISER:\n",
        "# resultat = exemple_traitement_pdf_complet(\"data/pdfs/votre_plan.pdf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.2 Exemple: Traitement de Tous les PDFs d'un Dossier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXEMPLE 2: TRAITEMENT BATCH DE PLUSIEURS PDFs\n",
        "# ============================================================================\n",
        "\n",
        "def exemple_traitement_batch(pdf_directory: str = \"data/pdfs\"):\n",
        "    \"\"\"\n",
        "    Traite tous les PDFs d'un r√©pertoire\n",
        "    \n",
        "    Usage:\n",
        "        exemple_traitement_batch(\"data/pdfs\")\n",
        "    \"\"\"\n",
        "    pdf_dir = Path(pdf_directory)\n",
        "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
        "    \n",
        "    if not pdf_files:\n",
        "        print(f\"‚ùå Aucun PDF trouv√© dans {pdf_directory}\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"üìö {len(pdf_files)} PDFs trouv√©s\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
        "        print(f\"\\nüìÑ [{idx}/{len(pdf_files)}] Traitement: {pdf_path.name}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        try:\n",
        "            # Extraction\n",
        "            result = process_pdf(str(pdf_path))\n",
        "            \n",
        "            if result:\n",
        "                # D√©tection sur premi√®re page de chaque PDF\n",
        "                if result['saved_paths']:\n",
        "                    annotations = labeler.label_image(result['saved_paths'][0])\n",
        "                    result['annotations'] = annotations\n",
        "                    results.append(result)\n",
        "                    \n",
        "                    print(f\"  ‚úÖ {len(annotations['detections'])} d√©tections\")\n",
        "                else:\n",
        "                    print(\"  ‚ö†Ô∏è Aucune image extraite\")\n",
        "            else:\n",
        "                print(\"  ‚ùå Erreur lors du traitement\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erreur: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"‚úÖ TRAITEMENT BATCH TERMIN√â: {len(results)}/{len(pdf_files)} PDFs trait√©s\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Statistiques globales\n",
        "    total_detections = sum(len(r.get('annotations', {}).get('detections', [])) for r in results)\n",
        "    print(f\"\\nüìä Statistiques globales:\")\n",
        "    print(f\"   - Total d√©tections: {total_detections}\")\n",
        "    print(f\"   - Moyenne par PDF: {total_detections/len(results) if results else 0:.1f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# D√âCOMMENTER POUR UTILISER:\n",
        "# tous_les_resultats = exemple_traitement_batch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.3 Exemple: D√©tection et Export Vectoriel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXEMPLE 3: D√âTECTION + EXPORT VECTORIEL\n",
        "# ============================================================================\n",
        "\n",
        "def exemple_detection_et_export(image_path: str, output_format: str = \"dxf\"):\n",
        "    \"\"\"\n",
        "    D√©tecte les √©l√©ments et exporte en format vectoriel\n",
        "    \n",
        "    Args:\n",
        "        image_path: Chemin vers l'image du plan\n",
        "        output_format: \"dxf\" ou \"ifc\"\n",
        "    \n",
        "    Usage:\n",
        "        exemple_detection_et_export(\"data/images/plan.png\", \"dxf\")\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üîç D√âTECTION ET EXPORT {output_format.upper()}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # D√©tection compl√®te\n",
        "    print(\"\\n1Ô∏è‚É£ D√©tection des √©l√©ments...\")\n",
        "    detections = labeler.predict_detection(image_path)\n",
        "    segmentations = labeler.predict_segmentation(image_path)\n",
        "    keypoints = labeler.predict_keypoints(image_path)\n",
        "    \n",
        "    annotations = {\n",
        "        'detections': detections,\n",
        "        'segmentations': segmentations,\n",
        "        'keypoints': keypoints\n",
        "    }\n",
        "    \n",
        "    print(f\"   ‚úÖ {len(detections)} d√©tections\")\n",
        "    print(f\"   ‚úÖ {len(segmentations)} segmentations\")\n",
        "    print(f\"   ‚úÖ {len(keypoints)} keypoints\")\n",
        "    \n",
        "    # Export (exemple simplifi√© - voir Section 7 pour l'impl√©mentation compl√®te)\n",
        "    print(f\"\\n2Ô∏è‚É£ Export en format {output_format.upper()}...\")\n",
        "    \n",
        "    if output_format.lower() == \"dxf\" and EZDXF_AVAILABLE:\n",
        "        try:\n",
        "            import ezdxf\n",
        "            doc = ezdxf.new('R2010')\n",
        "            msp = doc.modelspace()\n",
        "            \n",
        "            # Ajouter les √©l√©ments d√©tect√©s comme lignes/polygones\n",
        "            for det in detections:\n",
        "                bbox = det['bbox']\n",
        "                # Conversion coordonn√©es normalis√©es ‚Üí pixels ‚Üí m√®tres\n",
        "                # (√† adapter selon votre √©chelle)\n",
        "                x = bbox['x_center'] * 1000  # Exemple: √©chelle\n",
        "                y = bbox['y_center'] * 1000\n",
        "                w = bbox['width'] * 1000\n",
        "                h = bbox['height'] * 1000\n",
        "                \n",
        "                # Dessiner rectangle\n",
        "                msp.add_lwpolyline([\n",
        "                    (x - w/2, y - h/2),\n",
        "                    (x + w/2, y - h/2),\n",
        "                    (x + w/2, y + h/2),\n",
        "                    (x - w/2, y + h/2),\n",
        "                    (x - w/2, y - h/2)\n",
        "                ])\n",
        "            \n",
        "            output_path = config.OUTPUT_DIR / f\"{Path(image_path).stem}.dxf\"\n",
        "            doc.saveas(output_path)\n",
        "            print(f\"   ‚úÖ Fichier sauvegard√©: {output_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Erreur export DXF: {e}\")\n",
        "    \n",
        "    elif output_format.lower() == \"ifc\" and IFC_AVAILABLE:\n",
        "        print(\"   ‚ö†Ô∏è Export IFC - Voir Section 7 pour impl√©mentation compl√®te\")\n",
        "        # Impl√©mentation IFC plus complexe, voir Section 7\n",
        "    \n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Format {output_format} non disponible\")\n",
        "    \n",
        "    # Visualisation\n",
        "    print(\"\\n3Ô∏è‚É£ Visualisation...\")\n",
        "    fig = labeler.visualize_annotations(image_path, annotations)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ EXPORT TERMIN√â!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return annotations\n",
        "\n",
        "# D√âCOMMENTER POUR UTILISER:\n",
        "# annotations = exemple_detection_et_export(\"data/images/plan.png\", \"dxf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.4 Exemple: Dashboard Interactif avec Plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXEMPLE 4: DASHBOARD INTERACTIF\n",
        "# ============================================================================\n",
        "\n",
        "def exemple_dashboard_interactif(image_path: str, annotations: Dict):\n",
        "    \"\"\"\n",
        "    Cr√©e un dashboard interactif avec Plotly\n",
        "    \n",
        "    Usage:\n",
        "        annotations = labeler.label_image(\"data/images/plan.png\")\n",
        "        exemple_dashboard_interactif(\"data/images/plan.png\", annotations)\n",
        "    \"\"\"\n",
        "    print(\"üìä Cr√©ation du dashboard interactif...\")\n",
        "    \n",
        "    # Charger l'image\n",
        "    img = Image.open(image_path)\n",
        "    img_array = np.array(img)\n",
        "    \n",
        "    # Cr√©er figure avec subplots\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Plan Original', 'D√©tections', \n",
        "                       'Distribution des Classes', 'Confidences'),\n",
        "        specs=[[{\"type\": \"image\"}, {\"type\": \"image\"}],\n",
        "               [{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
        "    )\n",
        "    \n",
        "    # 1. Image originale\n",
        "    fig.add_trace(\n",
        "        go.Image(z=img_array),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 2. Image avec annotations\n",
        "    # Dessiner les bboxes sur l'image\n",
        "    annotated_img = img_array.copy()\n",
        "    for det in annotations['detections']:\n",
        "        bbox = det['bbox']\n",
        "        h, w = img_array.shape[:2]\n",
        "        \n",
        "        x1 = int((bbox['x_center'] - bbox['width']/2) * w)\n",
        "        y1 = int((bbox['y_center'] - bbox['height']/2) * h)\n",
        "        x2 = int((bbox['x_center'] + bbox['width']/2) * w)\n",
        "        y2 = int((bbox['y_center'] + bbox['height']/2) * h)\n",
        "        \n",
        "        # Dessiner rectangle (simplifi√©)\n",
        "        annotated_img[y1:y1+3, x1:x2] = [255, 0, 0]  # Rouge\n",
        "        annotated_img[y2-3:y2, x1:x2] = [255, 0, 0]\n",
        "        annotated_img[y1:y2, x1:x1+3] = [255, 0, 0]\n",
        "        annotated_img[y1:y2, x2-3:x2] = [255, 0, 0]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Image(z=annotated_img),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # 3. Distribution des classes\n",
        "    classes = [det['class_name'] for det in annotations['detections']]\n",
        "    class_counts = pd.Series(classes).value_counts()\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=class_counts.index,\n",
        "            y=class_counts.values,\n",
        "            name=\"Classes\"\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # 4. Histogramme des confidences\n",
        "    confidences = [det['confidence'] for det in annotations['detections']]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=confidences,\n",
        "            nbinsx=20,\n",
        "            name=\"Confidences\"\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # Mise √† jour layout\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        title_text=\"Dashboard Interactif - Extraction de Plans\",\n",
        "        showlegend=False\n",
        "    )\n",
        "    \n",
        "    # Afficher\n",
        "    fig.show()\n",
        "    \n",
        "    print(\"‚úÖ Dashboard cr√©√© et affich√©!\")\n",
        "\n",
        "# D√âCOMMENTER POUR UTILISER:\n",
        "# annotations = labeler.label_image(\"data/images/plan.png\")\n",
        "# exemple_dashboard_interactif(\"data/images/plan.png\", annotations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.5 R√©sum√©: Commandes Essentielles\n",
        "\n",
        "### üéØ Workflow Minimal (Copier-Coller)\n",
        "\n",
        "```python\n",
        "# 1. Setup (ex√©cuter Section 1 d'abord)\n",
        "\n",
        "# 2. Traiter un PDF\n",
        "pdf_path = \"data/pdfs/mon_plan.pdf\"\n",
        "result = process_pdf(pdf_path)\n",
        "\n",
        "# 3. D√©tecter les √©l√©ments\n",
        "annotations = labeler.label_image(result['saved_paths'][0])\n",
        "\n",
        "# 4. Visualiser\n",
        "labeler.visualize_annotations(result['saved_paths'][0], annotations)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### üìö Pour Plus d'Informations\n",
        "\n",
        "- **Guide complet**: Voir `GUIDE_UTILISATION.md`\n",
        "- **Prompt expert**: Voir `PROMPT_EXPERT.md`\n",
        "- **Documentation**: Chaque section du notebook contient des explications d√©taill√©es\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Checklist d'Utilisation\n",
        "\n",
        "- [ ] Section 1 ex√©cut√©e (Setup)\n",
        "- [ ] PDFs plac√©s dans `data/pdfs/`\n",
        "- [ ] Section 3 ex√©cut√©e (Extraction)\n",
        "- [ ] Section 5 ex√©cut√©e (Labellisation)\n",
        "- [ ] R√©sultats visualis√©s\n",
        "- [ ] (Optionnel) Section 4 ex√©cut√©e (G√©n√©ration donn√©es)\n",
        "- [ ] (Optionnel) Section 6 ex√©cut√©e (Entra√Ænement)\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Vous √™tes pr√™t √† utiliser le projet !**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PR√âPARATION DATASET POUR ENTRA√éNEMENT YOLO\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetPreparator:\n",
        "    \"\"\"Pr√©pare le dataset au format YOLO\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    \n",
        "    def create_yolo_structure(self, base_dir: Path):\n",
        "        \"\"\"Cr√©e la structure de r√©pertoires YOLO\"\"\"\n",
        "        dirs = {\n",
        "            'train': ['images', 'labels'],\n",
        "            'val': ['images', 'labels'],\n",
        "            'test': ['images', 'labels']\n",
        "        }\n",
        "        \n",
        "        for split, subdirs in dirs.items():\n",
        "            for subdir in subdirs:\n",
        "                (base_dir / split / subdir).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        print(f\"‚úÖ Structure YOLO cr√©√©e dans {base_dir}\")\n",
        "    \n",
        "    def split_dataset(self, image_paths: List[str], \n",
        "                     train_ratio: float = 0.7,\n",
        "                     val_ratio: float = 0.2,\n",
        "                     test_ratio: float = 0.1) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Divise le dataset en train/val/test\n",
        "        \n",
        "        Args:\n",
        "            image_paths: Liste des chemins d'images\n",
        "            train_ratio: Proportion train\n",
        "            val_ratio: Proportion validation\n",
        "            test_ratio: Proportion test\n",
        "        \"\"\"\n",
        "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 0.01, \"Ratios doivent sommer √† 1\"\n",
        "        \n",
        "        import random\n",
        "        random.shuffle(image_paths)\n",
        "        \n",
        "        n_total = len(image_paths)\n",
        "        n_train = int(n_total * train_ratio)\n",
        "        n_val = int(n_total * val_ratio)\n",
        "        \n",
        "        splits = {\n",
        "            'train': image_paths[:n_train],\n",
        "            'val': image_paths[n_train:n_train + n_val],\n",
        "            'test': image_paths[n_train + n_val:]\n",
        "        }\n",
        "        \n",
        "        print(f\"üìä Split dataset:\")\n",
        "        print(f\"  Train: {len(splits['train'])} ({len(splits['train'])/n_total*100:.1f}%)\")\n",
        "        print(f\"  Val: {len(splits['val'])} ({len(splits['val'])/n_total*100:.1f}%)\")\n",
        "        print(f\"  Test: {len(splits['test'])} ({len(splits['test'])/n_total*100:.1f}%)\")\n",
        "        \n",
        "        return splits\n",
        "    \n",
        "    def copy_to_yolo_structure(self, splits: Dict[str, List[str]], \n",
        "                               yolo_dir: Path, annotations_dir: Path):\n",
        "        \"\"\"Copie images et labels dans la structure YOLO\"\"\"\n",
        "        import shutil\n",
        "        \n",
        "        for split, image_paths in splits.items():\n",
        "            for img_path in image_paths:\n",
        "                # Copie image\n",
        "                img_name = Path(img_path).name\n",
        "                dst_img = yolo_dir / split / \"images\" / img_name\n",
        "                shutil.copy(img_path, dst_img)\n",
        "                \n",
        "                # Copie label (si existe)\n",
        "                label_name = Path(img_path).stem + \".txt\"\n",
        "                src_label = annotations_dir / label_name\n",
        "                if src_label.exists():\n",
        "                    dst_label = yolo_dir / split / \"labels\" / label_name\n",
        "                    shutil.copy(src_label, dst_label)\n",
        "        \n",
        "        print(f\"‚úÖ Dataset copi√© dans structure YOLO: {yolo_dir}\")\n",
        "    \n",
        "    def create_yolo_config(self, yolo_dir: Path, num_classes: int) -> str:\n",
        "        \"\"\"Cr√©e le fichier de configuration YOLO (data.yaml)\"\"\"\n",
        "        class_names = [self.config.CLASSES[i] for i in sorted(self.config.CLASSES.keys())]\n",
        "        \n",
        "        config_content = f\"\"\"\n",
        "# Dataset configuration for YOLO\n",
        "path: {yolo_dir.absolute()}\n",
        "train: train/images\n",
        "val: val/images\n",
        "test: test/images\n",
        "\n",
        "# Classes\n",
        "nc: {num_classes}\n",
        "names:\n",
        "\"\"\"\n",
        "        for idx, name in enumerate(class_names):\n",
        "            config_content += f\"  {idx}: {name}\\n\"\n",
        "        \n",
        "        config_path = yolo_dir / \"data.yaml\"\n",
        "        with open(config_path, 'w') as f:\n",
        "            f.write(config_content)\n",
        "        \n",
        "        print(f\"‚úÖ Configuration YOLO cr√©√©e: {config_path}\")\n",
        "        return str(config_path)\n",
        "\n",
        "# Initialisation\n",
        "dataset_prep = DatasetPreparator(config)\n",
        "\n",
        "print(\"‚úÖ Pr√©parateur de dataset initialis√©!\")\n",
        "print(\"üí° Pour pr√©parer le dataset:\")\n",
        "print(\"   1. Cr√©er structure: dataset_prep.create_yolo_structure(yolo_dir)\")\n",
        "print(\"   2. Split: splits = dataset_prep.split_dataset(image_paths)\")\n",
        "print(\"   3. Copier: dataset_prep.copy_to_yolo_structure(splits, yolo_dir, annotations_dir)\")\n",
        "print(\"   4. Config: dataset_prep.create_yolo_config(yolo_dir, num_classes)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
